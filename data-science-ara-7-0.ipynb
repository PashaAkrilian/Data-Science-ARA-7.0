{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":128328,"databundleVersionId":15445689,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3935.89029,"end_time":"2026-02-05T21:13:18.335672","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-05T20:07:42.445382","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"120cd859542a4ad4914a275aa970c3ce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1556656a169c433faf0fcce78ce0c18c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"223f1b968d2e43c7a09bad50be0acdf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1fa80fce055417791cace1a6c61f72c","IPY_MODEL_27a16559e0864698a232fe1fe00e2df0","IPY_MODEL_2b6e0b5f64fd476bbe7660a79f157fba"],"layout":"IPY_MODEL_cbeae11723de45daa7bbb1da1bdc1dc2","tabbable":null,"tooltip":null}},"27a16559e0864698a232fe1fe00e2df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b39bdab99ac142e2be09b746d904869f","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1556656a169c433faf0fcce78ce0c18c","tabbable":null,"tooltip":null,"value":40}},"2b6e0b5f64fd476bbe7660a79f157fba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_629cbd2586334b23a726eed97daf3659","placeholder":"​","style":"IPY_MODEL_64e3be10918f48c1a6898ddb584bb72f","tabbable":null,"tooltip":null,"value":" 40/40 [04:58&lt;00:00,  7.45s/it]"}},"629cbd2586334b23a726eed97daf3659":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e3be10918f48c1a6898ddb584bb72f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"70cc7b9087a64bed8c59a7024c7dd314":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b39bdab99ac142e2be09b746d904869f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbeae11723de45daa7bbb1da1bdc1dc2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1fa80fce055417791cace1a6c61f72c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_120cd859542a4ad4914a275aa970c3ce","placeholder":"​","style":"IPY_MODEL_70cc7b9087a64bed8c59a7024c7dd314","tabbable":null,"tooltip":null,"value":"Best trial: 36. Best value: 0.75214: 100%"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Understanding & Preparation","metadata":{"papermill":{"duration":0.003014,"end_time":"2026-02-05T20:07:45.219228","exception":false,"start_time":"2026-02-05T20:07:45.216214","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Data Understanding & Preparation (REVISED · LB-READY)\n# Purpose:\n# - Validate dataset integrity\n# - Quantify Dice risk factors (empty / tiny / fragmented)\n# - Derive data-driven priors for:\n#   • sampling strategy\n#   • min-area postprocess\n#   • threshold sweep\n# - Produce manifest for downstream stages\n# ============================================================\n\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTRAIN_IMG_DIR = DATA_ROOT / \"train/images\"\nTRAIN_MASK_DIR = DATA_ROOT / \"train/mask\"\nTEST_IMG_DIR  = DATA_ROOT / \"test/images\"\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n\n# -----------------------------\n# 1. LOAD FILES\n# -----------------------------\ntrain_images = sorted([p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntrain_masks  = sorted([p for p in TRAIN_MASK_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntest_images  = sorted([p for p in TEST_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n\nprint(f\"[INFO] Train images : {len(train_images)}\")\nprint(f\"[INFO] Train masks  : {len(train_masks)}\")\nprint(f\"[INFO] Test images  : {len(test_images)}\")\n\n# -----------------------------\n# 2. INDEX MASKS\n# -----------------------------\ndef extract_index(name: str):\n    m = re.search(r\"(\\d+)\", name)\n    return m.group(1) if m else None\n\nmask_index = {extract_index(m.stem): m for m in train_masks if extract_index(m.stem)}\n\n# -----------------------------\n# 3. PAIR IMAGE–MASK\n# -----------------------------\npairs = []\nfor img in train_images:\n    idx = extract_index(img.stem)\n    if idx in mask_index:\n        pairs.append({\n            \"image_path\": img,\n            \"mask_path\": mask_index[idx],\n            \"id\": idx\n        })\n\nassert len(pairs) > 0\nprint(f\"[INFO] Valid image-mask pairs: {len(pairs)}\")\n\n# -----------------------------\n# 4. MORPHOLOGY & DICE-RISK ANALYSIS\n# -----------------------------\nrecords = []\nall_component_areas = []\n\nfor p in tqdm(pairs, desc=\"Analyzing dataset\"):\n    mask = cv2.imread(str(p[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n    h, w = mask.shape\n    total_pixels = h * w\n\n    bin_mask = (mask == 255).astype(np.uint8)\n    pothole_pixels = bin_mask.sum()\n    area_ratio = pothole_pixels / total_pixels\n\n    num_labels, _, stats, _ = cv2.connectedComponentsWithStats(\n        bin_mask, connectivity=8\n    )\n\n    component_areas = stats[1:, cv2.CC_STAT_AREA] if num_labels > 1 else []\n    if len(component_areas) > 0:\n        all_component_areas.extend(component_areas.tolist())\n\n    # bucket for stratified sampling\n    if pothole_pixels == 0:\n        bucket = \"empty\"\n    elif area_ratio < 0.002:\n        bucket = \"tiny\"\n    elif area_ratio < 0.01:\n        bucket = \"small\"\n    elif area_ratio < 0.05:\n        bucket = \"medium\"\n    else:\n        bucket = \"large\"\n\n    records.append({\n        \"image\": p[\"image_path\"].name,\n        \"image_path\": str(p[\"image_path\"]),\n        \"mask_path\": str(p[\"mask_path\"]),\n        \"height\": h,\n        \"width\": w,\n        \"has_pothole\": int(pothole_pixels > 0),\n        \"area_ratio\": area_ratio,\n        \"total_pothole_pixels\": pothole_pixels,\n        \"num_components\": len(component_areas),\n        \"max_component_pixels\": component_areas.max() if len(component_areas) > 0 else 0,\n        \"bucket\": bucket,\n    })\n\ndf = pd.DataFrame(records)\n\n# -----------------------------\n# 5. CORE INSIGHTS\n# -----------------------------\nprint(\"\\n[INSIGHT] Pothole presence:\")\nprint(df[\"has_pothole\"].value_counts())\n\nprint(\"\\n[INSIGHT] Area ratio (% image):\")\nprint(df[\"area_ratio\"].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n\nprint(\"\\n[INSIGHT] Bucket distribution:\")\nprint(df[\"bucket\"].value_counts(normalize=True).round(3))\n\n# -----------------------------\n# 6. SMALL OBJECT ANALYSIS (POSTPROCESS PRIOR)\n# -----------------------------\ncomp_series = pd.Series(all_component_areas)\n\nprint(\"\\n[INSIGHT] Connected component area (px):\")\nprint(comp_series.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n\nMIN_AREA_PX = int(comp_series.quantile(0.10))\nprint(f\"\\n[LOCKED PRIOR] MIN_AREA_PX ≈ {MIN_AREA_PX}\")\n\n# -----------------------------\n# 7. DICE FEASIBILITY SIGNAL\n# -----------------------------\ntiny_ratio = (df[\"area_ratio\"] < 0.01).mean()\nprint(f\"\\n[FEASIBILITY] <1% area images: {tiny_ratio:.2%}\")\n\nif tiny_ratio > 0.6:\n    feasibility = \"HARD\"\nelif tiny_ratio > 0.4:\n    feasibility = \"MODERATE\"\nelse:\n    feasibility = \"FAVORABLE\"\n\nprint(f\"[FEASIBILITY STATUS] {feasibility}\")\n\n# -----------------------------\n# 8. THRESHOLD PRIOR\n# -----------------------------\nTHR_START, THR_END = 0.30, 0.45\nprint(\"\\n[LOCKED THRESHOLD PRIOR]\")\nprint(f\"Use sweep range: {THR_START:.2f} – {THR_END:.2f}\")\n\n# -----------------------------\n# 9. FINAL MANIFEST (DOWNSTREAM READY)\n# -----------------------------\ndf_manifest = df[[\n    \"image_path\",\n    \"mask_path\",\n    \"has_pothole\",\n    \"area_ratio\",\n    \"bucket\"\n]].copy()\n\nprint(f\"\\n[INFO] Final training samples: {len(df_manifest)}\")\n\nprint(\"\\n[STAGE 1 COMPLETE — LB-READY]\")\nprint(\"✓ Dataset validated\")\nprint(\"✓ Sampling buckets defined\")\nprint(\"✓ Min-area & threshold locked\")\nprint(\"✓ Manifest ready for STAGE 2/3\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T18:02:05.901954Z","iopub.execute_input":"2026-02-08T18:02:05.902226Z","iopub.status.idle":"2026-02-08T18:02:19.546586Z","shell.execute_reply.started":"2026-02-08T18:02:05.902194Z","shell.execute_reply":"2026-02-08T18:02:19.545951Z"},"papermill":{"duration":13.015336,"end_time":"2026-02-05T20:07:58.236972","exception":false,"start_time":"2026-02-05T20:07:45.221636","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[INFO] Train images : 498\n[INFO] Train masks  : 498\n[INFO] Test images  : 295\n[INFO] Valid image-mask pairs: 498\n","output_type":"stream"},{"name":"stderr","text":"Analyzing dataset: 100%|██████████| 498/498 [00:11<00:00, 41.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n[INSIGHT] Pothole presence:\nhas_pothole\n1    498\nName: count, dtype: int64\n\n[INSIGHT] Area ratio (% image):\ncount    498.000000\nmean       0.134860\nstd        0.128772\nmin        0.000235\n10%        0.007938\n25%        0.040943\n50%        0.091678\n75%        0.193834\n90%        0.329536\nmax        0.674005\nName: area_ratio, dtype: float64\n\n[INSIGHT] Bucket distribution:\nbucket\nlarge     0.691\nmedium    0.191\nsmall     0.066\ntiny      0.052\nName: proportion, dtype: float64\n\n[INSIGHT] Connected component area (px):\ncount    2.122000e+03\nmean     5.588544e+04\nstd      3.030841e+05\nmin      1.000000e+00\n10%      1.301000e+02\n25%      3.930000e+02\n50%      1.913000e+03\n75%      1.203275e+04\n90%      5.370160e+04\nmax      6.700584e+06\ndtype: float64\n\n[LOCKED PRIOR] MIN_AREA_PX ≈ 130\n\n[FEASIBILITY] <1% area images: 11.85%\n[FEASIBILITY STATUS] FAVORABLE\n\n[LOCKED THRESHOLD PRIOR]\nUse sweep range: 0.30 – 0.45\n\n[INFO] Final training samples: 498\n\n[STAGE 1 COMPLETE — LB-READY]\n✓ Dataset validated\n✓ Sampling buckets defined\n✓ Min-area & threshold locked\n✓ Manifest ready for STAGE 2/3\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Preprocessing & Data Augmentation","metadata":{"papermill":{"duration":0.005254,"end_time":"2026-02-05T20:07:58.247873","exception":false,"start_time":"2026-02-05T20:07:58.242619","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Preprocessing & Data Augmentation (FINAL · 0.80+)\n# ============================================================\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\n# ============================================================\n# TRAIN AUGMENTATION — MASK-AWARE & SHAPE-ROBUST\n# ============================================================\ntrain_transform_512 = A.Compose(\n    [\n        # --- FIXED RESOLUTION ---\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n\n        # --- MASK-AWARE SPATIAL FOCUS (CRITICAL) ---\n        A.CropNonEmptyMaskIfExists(\n            height=448,\n            width=448,\n            p=0.40,\n        ),\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n\n        # --- SAFE GEOMETRY ---\n        A.HorizontalFlip(p=0.5),\n\n        A.Affine(\n            scale=(0.97, 1.05),\n            translate_percent=(0.0, 0.03),\n            rotate=(-2.5, 2.5),\n            shear=(-1.5, 1.5),\n            interpolation=cv2.INTER_LINEAR,\n            mode=cv2.BORDER_REFLECT_101,\n            p=0.30,\n        ),\n\n        # --- SHAPE DEFORMATION (KEY FOR 0.80) ---\n        A.ElasticTransform(\n            alpha=20,\n            sigma=6,\n            alpha_affine=4,\n            border_mode=cv2.BORDER_REFLECT_101,\n            p=0.25,\n        ),\n\n        # --- PHOTOMETRIC ---\n        A.RandomBrightnessContrast(\n            brightness_limit=0.18,\n            contrast_limit=0.18,\n            p=0.65,\n        ),\n\n        A.HueSaturationValue(\n            hue_shift_limit=5,\n            sat_shift_limit=10,\n            val_shift_limit=5,\n            p=0.30,\n        ),\n\n        # --- SHADOW (CONSERVATIVE) ---\n        A.RandomShadow(\n            shadow_roi=(0, 0.6, 1, 1),\n            num_shadows_lower=1,\n            num_shadows_upper=1,\n            shadow_dimension=4,\n            p=0.15,\n        ),\n\n        # --- VERY MILD NOISE ---\n        A.OneOf(\n            [\n                A.GaussianBlur(blur_limit=3),\n                A.GaussNoise(var_limit=(4.0, 12.0)),\n            ],\n            p=0.12,\n        ),\n\n        # --- NORMALIZE ---\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n\n# ============================================================\n# VALID / TEST (STRICT)\n# ============================================================\nvalid_transform = A.Compose(\n    [\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n\ntest_transform = A.Compose(\n    [\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ]\n)\n\nprint(\"[STAGE 2 FINAL — SHAPE-AWARE & 0.80-READY]\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T18:02:19.548027Z","iopub.execute_input":"2026-02-08T18:02:19.548387Z","iopub.status.idle":"2026-02-08T18:02:25.251130Z","shell.execute_reply.started":"2026-02-08T18:02:19.548352Z","shell.execute_reply":"2026-02-08T18:02:25.250420Z"},"papermill":{"duration":6.054524,"end_time":"2026-02-05T20:08:04.307604","exception":false,"start_time":"2026-02-05T20:07:58.253080","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[STAGE 2 FINAL — SHAPE-AWARE & 0.80-READY]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/653124267.py:31: UserWarning: Argument(s) 'mode' are not valid for transform Affine\n  A.Affine(\n/tmp/ipykernel_55/653124267.py:42: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n  A.ElasticTransform(\n/tmp/ipykernel_55/653124267.py:65: UserWarning: Argument(s) 'num_shadows_lower, num_shadows_upper' are not valid for transform RandomShadow\n  A.RandomShadow(\n/tmp/ipykernel_55/653124267.py:77: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(4.0, 12.0)),\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Model Construction & Training","metadata":{"papermill":{"duration":0.005001,"end_time":"2026-02-05T20:08:04.317894","exception":false,"start_time":"2026-02-05T20:08:04.312893","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch==0.3.3 timm","metadata":{"execution":{"iopub.status.busy":"2026-02-08T18:02:25.251921Z","iopub.execute_input":"2026-02-08T18:02:25.252314Z","iopub.status.idle":"2026-02-08T18:02:36.054501Z","shell.execute_reply.started":"2026-02-08T18:02:25.252283Z","shell.execute_reply":"2026-02-08T18:02:36.053621Z"},"papermill":{"duration":10.935761,"end_time":"2026-02-05T20:08:15.258582","exception":false,"start_time":"2026-02-05T20:08:04.322821","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — PATCH + FULL IMAGE TRAINING (FINAL · ONE CELL)\n# UNet++ EffNet-B4 | SELF-CONTAINED | LB-SAFE\n# ============================================================\n\nimport random, re, cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nSEED = 42\nIMG_SIZE = 512\nPATCH = 320\n\nPATCH_EPOCHS = 25\nFULL_EPOCHS  = 18\n\nPATCH_BATCH = 6\nFULL_BATCH  = 4\nACCUM = 2\n\nLR_PATCH = 3e-4\nLR_FULL  = 1e-5\n\nFREEZE_EPOCHS = 6\nTHR_RANGE = np.linspace(0.35, 0.50, 7)\n\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nIMG_DIR = DATA_ROOT / \"train/images\"\nMSK_DIR = DATA_ROOT / \"train/mask\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nprint(\"Device:\", DEVICE)\n\n# ============================================================\n# BUILD MANIFEST\n# ============================================================\ndef extract_idx(name):\n    return re.search(r\"(\\d+)\", name).group(1)\n\npairs = []\nfor img in IMG_DIR.iterdir():\n    idx = extract_idx(img.name)\n    msk = MSK_DIR / f\"mask_{idx}.png\"\n    if msk.exists():\n        pairs.append((str(img), str(msk)))\n\ndf = pd.DataFrame(pairs, columns=[\"image_path\", \"mask_path\"])\ndf_train, df_val = train_test_split(\n    df, test_size=0.15, random_state=SEED, shuffle=True\n)\n\nprint(\"Train:\", len(df_train), \"| Val:\", len(df_val))\n\n# ============================================================\n# DATASETS\n# ============================================================\nclass PatchDataset(Dataset):\n    def __init__(self, df, tf):\n        self.df = df.reset_index(drop=True)\n        self.tf = tf\n\n    def __len__(self):\n        return len(self.df) * 3\n\n    def _safe_patch(self, img, mask, x1, y1):\n        h, w = img.shape[:2]\n        x1 = max(0, min(x1, w - PATCH))\n        y1 = max(0, min(y1, h - PATCH))\n        img_c = img[y1:y1+PATCH, x1:x1+PATCH]\n        mask_c = mask[y1:y1+PATCH, x1:x1+PATCH]\n\n        if img_c.shape[:2] != (PATCH, PATCH):\n            img_c = cv2.resize(img, (PATCH, PATCH))\n            mask_c = cv2.resize(mask, (PATCH, PATCH), interpolation=cv2.INTER_NEAREST)\n\n        return img_c, mask_c\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx % len(self.df)]\n        img = cv2.imread(row.image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(row.mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(np.uint8)\n\n        if mask.sum() > 0 and random.random() < 0.7:\n            ys, xs = np.where(mask > 0)\n            i = random.randint(0, len(xs)-1)\n            img, mask = self._safe_patch(\n                img, mask,\n                xs[i] - PATCH//2,\n                ys[i] - PATCH//2\n            )\n        else:\n            h, w = img.shape[:2]\n            if h >= PATCH and w >= PATCH:\n                x1 = random.randint(0, w - PATCH)\n                y1 = random.randint(0, h - PATCH)\n                img, mask = self._safe_patch(img, mask, x1, y1)\n            else:\n                img = cv2.resize(img, (PATCH, PATCH))\n                mask = cv2.resize(mask, (PATCH, PATCH), interpolation=cv2.INTER_NEAREST)\n\n        aug = self.tf(image=img, mask=mask)\n        return aug[\"image\"], aug[\"mask\"].unsqueeze(0).float()\n\n\nclass FullDataset(Dataset):\n    def __init__(self, df, tf):\n        self.df = df.reset_index(drop=True)\n        self.tf = tf\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = cv2.imread(row.image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(row.mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(np.float32)\n        aug = self.tf(image=img, mask=mask)\n        return aug[\"image\"], aug[\"mask\"].unsqueeze(0)\n\n# ============================================================\n# TRANSFORMS\n# ============================================================\nnorm = dict(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n\npatch_tf = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(0.15, 0.15, p=0.5),\n    A.Normalize(**norm),\n    ToTensorV2(),\n])\n\nfull_train_tf = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(**norm),\n    ToTensorV2(),\n])\n\nfull_val_tf = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(**norm),\n    ToTensorV2(),\n])\n\n# ============================================================\n# MODEL & LOSS\n# ============================================================\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n).to(DEVICE)\n\ndice = smp.losses.DiceLoss(mode=\"binary\", from_logits=True)\nfocal = smp.losses.FocalLoss(mode=\"binary\", gamma=2.0)\n\n# ============================================================\n# PHASE A — PATCH TRAINING\n# ============================================================\npatch_loader = DataLoader(\n    PatchDataset(df_train, patch_tf),\n    batch_size=PATCH_BATCH,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n)\n\nopt = AdamW(model.parameters(), lr=LR_PATCH, weight_decay=1e-4)\nsch = CosineAnnealingLR(opt, T_max=PATCH_EPOCHS)\n\nfor e in range(1, PATCH_EPOCHS+1):\n    model.train()\n    tot = 0\n    for x,y in tqdm(patch_loader, desc=f\"Patch {e}\"):\n        x,y = x.to(DEVICE), y.to(DEVICE)\n        opt.zero_grad()\n        loss = dice(model(x),y) + 0.5*focal(model(x),y)\n        loss.backward()\n        opt.step()\n        tot += loss.item()\n    sch.step()\n    print(f\"Patch Epoch {e:02d} | Loss {tot:.4f}\")\n\n# ============================================================\n# PHASE B — FULL FINETUNE\n# ============================================================\nfor p in model.encoder.parameters():\n    p.requires_grad = False\n\nopt = AdamW([\n    {\"params\": model.encoder.parameters(), \"lr\": LR_FULL*0.1},\n    {\"params\": model.decoder.parameters(), \"lr\": LR_FULL},\n], weight_decay=1e-4)\n\nsch = CosineAnnealingLR(opt, T_max=FULL_EPOCHS)\n\ntrain_loader = DataLoader(\n    FullDataset(df_train, full_train_tf),\n    batch_size=FULL_BATCH,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n)\n\nval_loader = DataLoader(\n    FullDataset(df_val, full_val_tf),\n    batch_size=FULL_BATCH,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n)\n\n@torch.no_grad()\ndef validate():\n    model.eval()\n    best = 0\n    for thr in THR_RANGE:\n        scores=[]\n        for x,y in val_loader:\n            x,y = x.to(DEVICE), y.to(DEVICE)\n            p = (torch.sigmoid(model(x))>thr).float()\n            inter=(p*y).sum((2,3))\n            union=p.sum((2,3))+y.sum((2,3))\n            scores.append(((2*inter+1e-7)/(union+1e-7)).mean().item())\n        best=max(best,np.mean(scores))\n    return best\n\nbest=0\nfor e in range(1, FULL_EPOCHS+1):\n    model.train()\n    if e==FREEZE_EPOCHS+1:\n        for p in model.encoder.parameters(): p.requires_grad=True\n        print(\"[INFO] Encoder unfrozen\")\n\n    opt.zero_grad()\n    for i,(x,y) in enumerate(tqdm(train_loader, desc=f\"Full {e}\")):\n        x,y=x.to(DEVICE),y.to(DEVICE)\n        loss=(dice(model(x),y)+0.5*focal(model(x),y))/ACCUM\n        loss.backward()\n        if (i+1)%ACCUM==0:\n            opt.step(); opt.zero_grad()\n\n    sch.step()\n    vd=validate()\n    print(f\"Full Epoch {e:02d} | ValDice {vd:.4f}\")\n    if vd>best:\n        best=vd\n        torch.save(model.state_dict(),\"/kaggle/working/unetpp_best.pt\")\n        print(\">> Best saved\")\n\nprint(f\"\\n[BEST VAL DICE] {best:.4f}\")\nprint(\"[STAGE 3 DONE — STABLE & READY]\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T18:02:36.056004Z","iopub.execute_input":"2026-02-08T18:02:36.056373Z","iopub.status.idle":"2026-02-08T20:04:14.054590Z","shell.execute_reply.started":"2026-02-08T18:02:36.056331Z","shell.execute_reply":"2026-02-08T20:04:14.053720Z"},"papermill":{"duration":3531.638821,"end_time":"2026-02-05T21:07:06.903369","exception":false,"start_time":"2026-02-05T20:08:15.264548","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nTrain: 423 | Val: 75\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 74.4M/74.4M [00:00<00:00, 155MB/s] \nPatch 1: 100%|██████████| 212/212 [02:51<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 01 | Loss 107.0744\n","output_type":"stream"},{"name":"stderr","text":"Patch 2: 100%|██████████| 212/212 [02:57<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 02 | Loss 82.7072\n","output_type":"stream"},{"name":"stderr","text":"Patch 3: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 03 | Loss 70.3081\n","output_type":"stream"},{"name":"stderr","text":"Patch 4: 100%|██████████| 212/212 [02:57<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 04 | Loss 65.3861\n","output_type":"stream"},{"name":"stderr","text":"Patch 5: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 05 | Loss 57.6771\n","output_type":"stream"},{"name":"stderr","text":"Patch 6: 100%|██████████| 212/212 [02:56<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 06 | Loss 59.2744\n","output_type":"stream"},{"name":"stderr","text":"Patch 7: 100%|██████████| 212/212 [02:57<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 07 | Loss 53.1973\n","output_type":"stream"},{"name":"stderr","text":"Patch 8: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 08 | Loss 49.9093\n","output_type":"stream"},{"name":"stderr","text":"Patch 9: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 09 | Loss 42.8931\n","output_type":"stream"},{"name":"stderr","text":"Patch 10: 100%|██████████| 212/212 [02:56<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 10 | Loss 47.1722\n","output_type":"stream"},{"name":"stderr","text":"Patch 11: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 11 | Loss 45.2382\n","output_type":"stream"},{"name":"stderr","text":"Patch 12: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 12 | Loss 41.1487\n","output_type":"stream"},{"name":"stderr","text":"Patch 13: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 13 | Loss 38.3622\n","output_type":"stream"},{"name":"stderr","text":"Patch 14: 100%|██████████| 212/212 [02:56<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 14 | Loss 37.5250\n","output_type":"stream"},{"name":"stderr","text":"Patch 15: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 15 | Loss 35.2285\n","output_type":"stream"},{"name":"stderr","text":"Patch 16: 100%|██████████| 212/212 [02:57<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 16 | Loss 35.8676\n","output_type":"stream"},{"name":"stderr","text":"Patch 17: 100%|██████████| 212/212 [02:57<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 17 | Loss 31.5309\n","output_type":"stream"},{"name":"stderr","text":"Patch 18: 100%|██████████| 212/212 [02:58<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 18 | Loss 32.5686\n","output_type":"stream"},{"name":"stderr","text":"Patch 19: 100%|██████████| 212/212 [02:58<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 19 | Loss 31.4118\n","output_type":"stream"},{"name":"stderr","text":"Patch 20: 100%|██████████| 212/212 [02:58<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 20 | Loss 31.1632\n","output_type":"stream"},{"name":"stderr","text":"Patch 21: 100%|██████████| 212/212 [02:57<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 21 | Loss 29.2109\n","output_type":"stream"},{"name":"stderr","text":"Patch 22: 100%|██████████| 212/212 [02:58<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 22 | Loss 30.6353\n","output_type":"stream"},{"name":"stderr","text":"Patch 23: 100%|██████████| 212/212 [02:58<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 23 | Loss 29.9359\n","output_type":"stream"},{"name":"stderr","text":"Patch 24: 100%|██████████| 212/212 [02:57<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 24 | Loss 28.0355\n","output_type":"stream"},{"name":"stderr","text":"Patch 25: 100%|██████████| 212/212 [02:58<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 25 | Loss 27.3739\n","output_type":"stream"},{"name":"stderr","text":"Full 1: 100%|██████████| 106/106 [01:24<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 01 | ValDice 0.7318\n>> Best saved\n","output_type":"stream"},{"name":"stderr","text":"Full 2: 100%|██████████| 106/106 [01:24<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 02 | ValDice 0.7316\n","output_type":"stream"},{"name":"stderr","text":"Full 3: 100%|██████████| 106/106 [01:24<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 03 | ValDice 0.7319\n>> Best saved\n","output_type":"stream"},{"name":"stderr","text":"Full 4: 100%|██████████| 106/106 [01:24<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 04 | ValDice 0.7327\n>> Best saved\n","output_type":"stream"},{"name":"stderr","text":"Full 5: 100%|██████████| 106/106 [01:24<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 05 | ValDice 0.7338\n>> Best saved\n","output_type":"stream"},{"name":"stderr","text":"Full 6: 100%|██████████| 106/106 [01:24<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 06 | ValDice 0.7319\n[INFO] Encoder unfrozen\n","output_type":"stream"},{"name":"stderr","text":"Full 7: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 07 | ValDice 0.7290\n","output_type":"stream"},{"name":"stderr","text":"Full 8: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 08 | ValDice 0.7318\n","output_type":"stream"},{"name":"stderr","text":"Full 9: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 09 | ValDice 0.7317\n","output_type":"stream"},{"name":"stderr","text":"Full 10: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 10 | ValDice 0.7315\n","output_type":"stream"},{"name":"stderr","text":"Full 11: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 11 | ValDice 0.7321\n","output_type":"stream"},{"name":"stderr","text":"Full 12: 100%|██████████| 106/106 [02:34<00:00,  1.46s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 12 | ValDice 0.7314\n","output_type":"stream"},{"name":"stderr","text":"Full 13: 100%|██████████| 106/106 [02:34<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 13 | ValDice 0.7336\n","output_type":"stream"},{"name":"stderr","text":"Full 14: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 14 | ValDice 0.7337\n","output_type":"stream"},{"name":"stderr","text":"Full 15: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 15 | ValDice 0.7321\n","output_type":"stream"},{"name":"stderr","text":"Full 16: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 16 | ValDice 0.7333\n","output_type":"stream"},{"name":"stderr","text":"Full 17: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 17 | ValDice 0.7307\n","output_type":"stream"},{"name":"stderr","text":"Full 18: 100%|██████████| 106/106 [02:33<00:00,  1.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 18 | ValDice 0.7311\n\n[BEST VAL DICE] 0.7338\n[STAGE 3 DONE — STABLE & READY]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Optimization, Validation & Refinement","metadata":{"papermill":{"duration":0.188839,"end_time":"2026-02-05T21:07:07.280193","exception":false,"start_time":"2026-02-05T21:07:07.091354","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Optimization & Refinement (FINAL · 0.80 SAFE)\n# UNet++ ONLY | Patch-aware | Dice-correct\n# ============================================================\n\n!pip install -q optuna\n\nimport optuna\nimport numpy as np\nimport torch\nimport cv2\nfrom tqdm import tqdm\n\nimport segmentation_models_pytorch as smp\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# -----------------------------\n# DEVICE\n# -----------------------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\n# ============================================================\n# VALIDATION SET (FROM STAGE 3)\n# ============================================================\ndf_val = df_val.reset_index(drop=True)\n\n# ============================================================\n# DATASET\n# ============================================================\nclass ValDataset(Dataset):\n    def __init__(self, df, tf):\n        self.df = df\n        self.tf = tf\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.df.loc[idx, \"image_path\"])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.df.loc[idx, \"mask_path\"], cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(np.uint8)\n        aug = self.tf(image=img, mask=mask)\n        return aug[\"image\"], aug[\"mask\"]\n\n# ============================================================\n# TRANSFORM (STRICT)\n# ============================================================\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\nval_tf = A.Compose([\n    A.Resize(512, 512),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nval_loader = DataLoader(\n    ValDataset(df_val, val_tf),\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n)\n\n# ============================================================\n# LOAD MODEL\n# ============================================================\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=None,\n    in_channels=3,\n    classes=1,\n).to(DEVICE)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/unetpp_best.pt\", map_location=DEVICE))\nmodel.eval()\n\nprint(\"[INFO] UNet++ loaded\")\n\n# ============================================================\n# COMPUTE MIN_AREA RANGE (PATCH-AWARE)\n# ============================================================\nareas = []\nfor p in df_val[\"mask_path\"]:\n    m = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n    m = (m == 255).astype(np.uint8)\n    n, _, stats, _ = cv2.connectedComponentsWithStats(m, connectivity=8)\n    for i in range(1, n):\n        areas.append(stats[i, cv2.CC_STAT_AREA])\n\nareas = np.array(areas)\n\nMIN_AREA_LO = int(np.percentile(areas, 10))\nMIN_AREA_HI = int(np.percentile(areas, 35))\n\nprint(f\"[INFO] min_area range: {MIN_AREA_LO} – {MIN_AREA_HI}\")\n\n# ============================================================\n# DICE (CORRECT & FAIR)\n# ============================================================\ndef dice_correct(pred, gt, eps=1e-7):\n    if gt.sum() == 0 and pred.sum() == 0:\n        return 1.0\n    if gt.sum() == 0 and pred.sum() > 0:\n        return 0.0\n    inter = (pred * gt).sum()\n    union = pred.sum() + gt.sum()\n    return (2 * inter + eps) / (union + eps)\n\ndef remove_small(mask, min_area):\n    n, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    out = np.zeros_like(mask)\n    for i in range(1, n):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            out[labels == i] = 1\n    return out\n\n# ============================================================\n# OPTUNA OBJECTIVE\n# ============================================================\ndef objective(trial):\n    thr = trial.suggest_float(\"threshold\", 0.38, 0.55)\n    min_area = trial.suggest_int(\"min_area\", MIN_AREA_LO, MIN_AREA_HI, step=10)\n\n    scores = []\n\n    with torch.no_grad():\n        for imgs, masks in val_loader:\n            imgs = imgs.to(DEVICE)\n            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n            masks = masks.numpy()\n\n            for i in range(len(probs)):\n                p = (probs[i, 0] > thr).astype(np.uint8)\n                p = remove_small(p, min_area)\n                scores.append(dice_correct(p, masks[i]))\n\n    return float(np.mean(scores))\n\n# ============================================================\n# RUN OPTUNA\n# ============================================================\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=30, show_progress_bar=True)\n\nbest = study.best_params\n\nprint(\"\\n[OPTUNA BEST CONFIG — FINAL]\")\nfor k, v in best.items():\n    print(f\"{k}: {v}\")\nprint(f\"Validation Dice: {study.best_value:.4f}\")\n\n# ============================================================\n# EXPORT CONFIG\n# ============================================================\nOPT_CONFIG = {\n    \"weights\": {\"unetpp\": 1.0},\n    \"threshold\": best[\"threshold\"],\n    \"min_area\": best[\"min_area\"],\n}\n\nprint(\"\\n[STAGE 4 COMPLETE — 0.80 READY]\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T20:04:14.056123Z","iopub.execute_input":"2026-02-08T20:04:14.056920Z","iopub.status.idle":"2026-02-08T20:06:23.209166Z","shell.execute_reply.started":"2026-02-08T20:04:14.056886Z","shell.execute_reply":"2026-02-08T20:06:23.206608Z"},"papermill":{"duration":303.869045,"end_time":"2026-02-05T21:12:11.335789","exception":false,"start_time":"2026-02-05T21:07:07.466744","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Device: cuda\n[INFO] UNet++ loaded\n","output_type":"stream"},{"name":"stderr","text":"[I 2026-02-08 20:04:19,338] A new study created in memory with name: no-name-2570321e-40c0-48b4-bf8a-5cb9e109a493\n","output_type":"stream"},{"name":"stdout","text":"[INFO] min_area range: 118 – 1066\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3b5546c1fe4d8cb3aca83ea2685be0"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/optuna/distributions.py:684: UserWarning: The distribution is specified by [118, 1066] and step=10, but the range is not divisible by `step`. It will be replaced with [118, 1058].\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[I 2026-02-08 20:04:23,461] Trial 0 finished with value: 0.6860239981126488 and parameters: {'threshold': 0.5422901917669363, 'min_area': 708}. Best is trial 0 with value: 0.6860239981126488.\n[I 2026-02-08 20:04:27,536] Trial 1 finished with value: 0.7215595527199858 and parameters: {'threshold': 0.39535946824258306, 'min_area': 138}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:31,627] Trial 2 finished with value: 0.6839207068951452 and parameters: {'threshold': 0.5314836935548741, 'min_area': 888}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:35,738] Trial 3 finished with value: 0.6919621660962322 and parameters: {'threshold': 0.5361355428490915, 'min_area': 468}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:39,885] Trial 4 finished with value: 0.7082637628016039 and parameters: {'threshold': 0.4378812197618113, 'min_area': 268}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:44,064] Trial 5 finished with value: 0.7174101153203556 and parameters: {'threshold': 0.4233092405973384, 'min_area': 148}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:48,240] Trial 6 finished with value: 0.6930900663731072 and parameters: {'threshold': 0.433901424617438, 'min_area': 648}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:52,409] Trial 7 finished with value: 0.7076594778457256 and parameters: {'threshold': 0.4633408467135109, 'min_area': 368}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:04:56,562] Trial 8 finished with value: 0.7081196656682303 and parameters: {'threshold': 0.39423721055321725, 'min_area': 318}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:00,686] Trial 9 finished with value: 0.6922937102974899 and parameters: {'threshold': 0.39806971917182205, 'min_area': 638}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:04,805] Trial 10 finished with value: 0.6842001065602413 and parameters: {'threshold': 0.48601321078289744, 'min_area': 998}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:08,970] Trial 11 finished with value: 0.7212437772401139 and parameters: {'threshold': 0.3818148098808475, 'min_area': 138}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:13,125] Trial 12 finished with value: 0.7211859269906368 and parameters: {'threshold': 0.3874910836990184, 'min_area': 128}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:17,301] Trial 13 finished with value: 0.7205471495529907 and parameters: {'threshold': 0.3808075106777098, 'min_area': 118}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:21,408] Trial 14 finished with value: 0.701944075314561 and parameters: {'threshold': 0.4066735295439443, 'min_area': 488}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:25,521] Trial 15 finished with value: 0.716613613452643 and parameters: {'threshold': 0.46571455184419536, 'min_area': 228}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:29,622] Trial 16 finished with value: 0.7026862836796477 and parameters: {'threshold': 0.4173687356571291, 'min_area': 418}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:33,711] Trial 17 finished with value: 0.6856923829203742 and parameters: {'threshold': 0.49931064322517094, 'min_area': 798}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:37,798] Trial 18 finished with value: 0.7166898429929046 and parameters: {'threshold': 0.4418684499848826, 'min_area': 238}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:41,898] Trial 19 finished with value: 0.6936462658775251 and parameters: {'threshold': 0.40983845916481654, 'min_area': 528}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:46,003] Trial 20 finished with value: 0.7079732221405926 and parameters: {'threshold': 0.45768792908635997, 'min_area': 318}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:50,129] Trial 21 finished with value: 0.7181534829373278 and parameters: {'threshold': 0.38092275881367105, 'min_area': 178}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:54,261] Trial 22 finished with value: 0.7181714482046799 and parameters: {'threshold': 0.39120216545671155, 'min_area': 188}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:05:58,397] Trial 23 finished with value: 0.7214625313703072 and parameters: {'threshold': 0.4002653763849886, 'min_area': 128}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:06:02,532] Trial 24 finished with value: 0.7079940509342282 and parameters: {'threshold': 0.40404778166415684, 'min_area': 348}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:06:06,700] Trial 25 finished with value: 0.7167296074754756 and parameters: {'threshold': 0.4233924349567712, 'min_area': 248}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:06:10,827] Trial 26 finished with value: 0.7079320475251685 and parameters: {'threshold': 0.4072455009235176, 'min_area': 278}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:06:14,936] Trial 27 finished with value: 0.7028881063441529 and parameters: {'threshold': 0.44889578887839754, 'min_area': 408}. Best is trial 1 with value: 0.7215595527199858.\n[I 2026-02-08 20:06:19,060] Trial 28 finished with value: 0.7215678908328818 and parameters: {'threshold': 0.4202647648182623, 'min_area': 118}. Best is trial 28 with value: 0.7215678908328818.\n[I 2026-02-08 20:06:23,197] Trial 29 finished with value: 0.7180112429822313 and parameters: {'threshold': 0.4265572918644013, 'min_area': 198}. Best is trial 28 with value: 0.7215678908328818.\n\n[OPTUNA BEST CONFIG — FINAL]\nthreshold: 0.4202647648182623\nmin_area: 118\nValidation Dice: 0.7216\n\n[STAGE 4 COMPLETE — 0.80 READY]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Inference, Encoding & Submission","metadata":{"papermill":{"duration":0.18696,"end_time":"2026-02-05T21:12:11.715168","exception":false,"start_time":"2026-02-05T21:12:11.528208","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — FINAL INFERENCE, RLE & SUBMISSION (0.80 SAFE)\n# UNet++ ONLY | Patch-aware | Correct postprocess\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport cv2\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTEST_IMG_DIR = DATA_ROOT / \"test/images\"\nSAMPLE_SUB = Path(\"/kaggle/input/data-science-ara-7-0/sample_submission.csv\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTHRESHOLD = OPT_CONFIG[\"threshold\"]\nMIN_AREA_512 = OPT_CONFIG[\"min_area\"]\n\nIMG_SIZE = 512\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\n# -----------------------------\n# LOAD MODEL\n# -----------------------------\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=None,\n    in_channels=3,\n    classes=1,\n).to(DEVICE)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/unetpp_best.pt\", map_location=DEVICE))\nmodel.eval()\n\nprint(\"[INFO] UNet++ loaded\")\n\n# -----------------------------\n# TRANSFORM (512 ONLY)\n# -----------------------------\ninfer_tf = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\n# -----------------------------\n# RLE ENCODER (OFFICIAL)\n# -----------------------------\ndef encode_rle(mask: np.ndarray) -> str:\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[0::2]\n    return \" \".join(map(str, runs))\n\n# -----------------------------\n# POSTPROCESS (512 SPACE ONLY)\n# -----------------------------\ndef remove_small(mask, min_area):\n    n, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    out = np.zeros_like(mask, dtype=np.uint8)\n    for i in range(1, n):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            out[labels == i] = 1\n    return out\n\n# -----------------------------\n# INFERENCE\n# -----------------------------\nrecords = []\ntest_images = sorted(TEST_IMG_DIR.glob(\"*.jpg\"))\n\nwith torch.no_grad():\n    for img_path in tqdm(test_images, desc=\"Final Inference\"):\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h0, w0 = img.shape[:2]\n\n        x = infer_tf(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n        x_f = torch.flip(x, dims=[3])\n\n        # ---- forward + HFlip TTA ----\n        p  = torch.sigmoid(model(x))\n        p_f = torch.flip(torch.sigmoid(model(x_f)), dims=[3])\n        prob_512 = ((p + p_f) / 2.0)[0, 0].cpu().numpy()\n\n        # ---- GLOBAL LOW-CONFIDENCE GUARD ----\n        if prob_512.max() < THRESHOLD * 0.85:\n            pred_512 = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n        else:\n            pred_512 = (prob_512 > THRESHOLD).astype(np.uint8)\n            pred_512 = remove_small(pred_512, MIN_AREA_512)\n\n        # ---- resize to original ----\n        pred = cv2.resize(\n            pred_512, (w0, h0), interpolation=cv2.INTER_NEAREST\n        )\n\n        rle = \"\" if pred.sum() == 0 else encode_rle(pred)\n\n        records.append({\n            \"ImageId\": img_path.name,\n            \"rle\": rle\n        })\n\n# -----------------------------\n# SUBMISSION\n# -----------------------------\ndf_sub = pd.DataFrame(records)\ndf_sample = pd.read_csv(SAMPLE_SUB)\ndf_sub = df_sub[df_sample.columns.tolist()]\n\nOUT_SUB = \"/kaggle/working/submission.csv\"\ndf_sub.to_csv(OUT_SUB, index=False)\n\nprint(\"\\n[STAGE 5 COMPLETE — 0.80 READY SUBMISSION]\")\nprint(\"Saved to:\", OUT_SUB)\nprint(\"Total:\", len(df_sub))\nprint(\"Empty RLE:\", (df_sub['rle'] == '').sum())\nprint(df_sub.head())\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T20:06:23.211795Z","iopub.execute_input":"2026-02-08T20:06:23.212170Z","iopub.status.idle":"2026-02-08T20:06:58.244650Z","shell.execute_reply.started":"2026-02-08T20:06:23.212120Z","shell.execute_reply":"2026-02-08T20:06:58.243812Z"},"papermill":{"duration":62.681733,"end_time":"2026-02-05T21:13:14.583725","exception":false,"start_time":"2026-02-05T21:12:11.901992","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[INFO] UNet++ loaded\n","output_type":"stream"},{"name":"stderr","text":"Final Inference: 100%|██████████| 295/295 [00:34<00:00,  8.57it/s]","output_type":"stream"},{"name":"stdout","text":"\n[STAGE 5 COMPLETE — 0.80 READY SUBMISSION]\nSaved to: /kaggle/working/submission.csv\nTotal: 295\nEmpty RLE: 0\n        ImageId                                                rle\n0  test_001.jpg  4644 1 4942 4 5242 5 5540 7 5840 7 6139 9 6439...\n1  test_002.jpg  65208 10 65928 10 66646 13 67366 15 68086 15 6...\n2  test_003.jpg  534965 4 537261 4 539557 4 541853 4 544149 4 5...\n3  test_004.jpg  34951 2 35251 3 35550 6 35850 7 36150 10 36449...\n4  test_005.jpg  50530 5 50829 7 51127 10 51427 10 51726 13 520...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6}]}