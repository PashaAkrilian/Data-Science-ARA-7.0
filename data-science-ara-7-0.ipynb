{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":15445689,"sourceId":128328,"sourceType":"competition"}],"dockerImageVersionId":31260,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":3935.89029,"end_time":"2026-02-05T21:13:18.335672","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-05T20:07:42.445382","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"120cd859542a4ad4914a275aa970c3ce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1556656a169c433faf0fcce78ce0c18c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"223f1b968d2e43c7a09bad50be0acdf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1fa80fce055417791cace1a6c61f72c","IPY_MODEL_27a16559e0864698a232fe1fe00e2df0","IPY_MODEL_2b6e0b5f64fd476bbe7660a79f157fba"],"layout":"IPY_MODEL_cbeae11723de45daa7bbb1da1bdc1dc2","tabbable":null,"tooltip":null}},"27a16559e0864698a232fe1fe00e2df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b39bdab99ac142e2be09b746d904869f","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1556656a169c433faf0fcce78ce0c18c","tabbable":null,"tooltip":null,"value":40}},"2b6e0b5f64fd476bbe7660a79f157fba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_629cbd2586334b23a726eed97daf3659","placeholder":"â€‹","style":"IPY_MODEL_64e3be10918f48c1a6898ddb584bb72f","tabbable":null,"tooltip":null,"value":"â€‡40/40â€‡[04:58&lt;00:00,â€‡â€‡7.45s/it]"}},"629cbd2586334b23a726eed97daf3659":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e3be10918f48c1a6898ddb584bb72f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"70cc7b9087a64bed8c59a7024c7dd314":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b39bdab99ac142e2be09b746d904869f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbeae11723de45daa7bbb1da1bdc1dc2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1fa80fce055417791cace1a6c61f72c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_120cd859542a4ad4914a275aa970c3ce","placeholder":"â€‹","style":"IPY_MODEL_70cc7b9087a64bed8c59a7024c7dd314","tabbable":null,"tooltip":null,"value":"Bestâ€‡trial:â€‡36.â€‡Bestâ€‡value:â€‡0.75214:â€‡100%"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"6e13b858","cell_type":"markdown","source":"# Data Understanding & Preparation","metadata":{"papermill":{"duration":0.003014,"end_time":"2026-02-05T20:07:45.219228","exception":false,"start_time":"2026-02-05T20:07:45.216214","status":"completed"},"tags":[]}},{"id":"94b987a4","cell_type":"code","source":"# ============================================================\n# STAGE 1 â€” Data Understanding & Preparation (REVISED Â· LB-READY)\n# Purpose:\n# - Validate dataset integrity\n# - Quantify Dice risk factors (empty / tiny / fragmented)\n# - Derive data-driven priors for:\n#   â€¢ sampling strategy\n#   â€¢ min-area postprocess\n#   â€¢ threshold sweep\n# - Produce manifest for downstream stages\n# ============================================================\n\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTRAIN_IMG_DIR = DATA_ROOT / \"train/images\"\nTRAIN_MASK_DIR = DATA_ROOT / \"train/mask\"\nTEST_IMG_DIR  = DATA_ROOT / \"test/images\"\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n\n# -----------------------------\n# 1. LOAD FILES\n# -----------------------------\ntrain_images = sorted([p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntrain_masks  = sorted([p for p in TRAIN_MASK_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntest_images  = sorted([p for p in TEST_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n\nprint(f\"[INFO] Train images : {len(train_images)}\")\nprint(f\"[INFO] Train masks  : {len(train_masks)}\")\nprint(f\"[INFO] Test images  : {len(test_images)}\")\n\n# -----------------------------\n# 2. INDEX MASKS\n# -----------------------------\ndef extract_index(name: str):\n    m = re.search(r\"(\\d+)\", name)\n    return m.group(1) if m else None\n\nmask_index = {extract_index(m.stem): m for m in train_masks if extract_index(m.stem)}\n\n# -----------------------------\n# 3. PAIR IMAGEâ€“MASK\n# -----------------------------\npairs = []\nfor img in train_images:\n    idx = extract_index(img.stem)\n    if idx in mask_index:\n        pairs.append({\n            \"image_path\": img,\n            \"mask_path\": mask_index[idx],\n            \"id\": idx\n        })\n\nassert len(pairs) > 0\nprint(f\"[INFO] Valid image-mask pairs: {len(pairs)}\")\n\n# -----------------------------\n# 4. MORPHOLOGY & DICE-RISK ANALYSIS\n# -----------------------------\nrecords = []\nall_component_areas = []\n\nfor p in tqdm(pairs, desc=\"Analyzing dataset\"):\n    mask = cv2.imread(str(p[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n    h, w = mask.shape\n    total_pixels = h * w\n\n    bin_mask = (mask == 255).astype(np.uint8)\n    pothole_pixels = bin_mask.sum()\n    area_ratio = pothole_pixels / total_pixels\n\n    num_labels, _, stats, _ = cv2.connectedComponentsWithStats(\n        bin_mask, connectivity=8\n    )\n\n    component_areas = stats[1:, cv2.CC_STAT_AREA] if num_labels > 1 else []\n    if len(component_areas) > 0:\n        all_component_areas.extend(component_areas.tolist())\n\n    # bucket for stratified sampling\n    if pothole_pixels == 0:\n        bucket = \"empty\"\n    elif area_ratio < 0.002:\n        bucket = \"tiny\"\n    elif area_ratio < 0.01:\n        bucket = \"small\"\n    elif area_ratio < 0.05:\n        bucket = \"medium\"\n    else:\n        bucket = \"large\"\n\n    records.append({\n        \"image\": p[\"image_path\"].name,\n        \"image_path\": str(p[\"image_path\"]),\n        \"mask_path\": str(p[\"mask_path\"]),\n        \"height\": h,\n        \"width\": w,\n        \"has_pothole\": int(pothole_pixels > 0),\n        \"area_ratio\": area_ratio,\n        \"total_pothole_pixels\": pothole_pixels,\n        \"num_components\": len(component_areas),\n        \"max_component_pixels\": component_areas.max() if len(component_areas) > 0 else 0,\n        \"bucket\": bucket,\n    })\n\ndf = pd.DataFrame(records)\n\n# -----------------------------\n# 5. CORE INSIGHTS\n# -----------------------------\nprint(\"\\n[INSIGHT] Pothole presence:\")\nprint(df[\"has_pothole\"].value_counts())\n\nprint(\"\\n[INSIGHT] Area ratio (% image):\")\nprint(df[\"area_ratio\"].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n\nprint(\"\\n[INSIGHT] Bucket distribution:\")\nprint(df[\"bucket\"].value_counts(normalize=True).round(3))\n\n# -----------------------------\n# 6. SMALL OBJECT ANALYSIS (POSTPROCESS PRIOR)\n# -----------------------------\ncomp_series = pd.Series(all_component_areas)\n\nprint(\"\\n[INSIGHT] Connected component area (px):\")\nprint(comp_series.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n\nMIN_AREA_PX = int(comp_series.quantile(0.10))\nprint(f\"\\n[LOCKED PRIOR] MIN_AREA_PX â‰ˆ {MIN_AREA_PX}\")\n\n# -----------------------------\n# 7. DICE FEASIBILITY SIGNAL\n# -----------------------------\ntiny_ratio = (df[\"area_ratio\"] < 0.01).mean()\nprint(f\"\\n[FEASIBILITY] <1% area images: {tiny_ratio:.2%}\")\n\nif tiny_ratio > 0.6:\n    feasibility = \"HARD\"\nelif tiny_ratio > 0.4:\n    feasibility = \"MODERATE\"\nelse:\n    feasibility = \"FAVORABLE\"\n\nprint(f\"[FEASIBILITY STATUS] {feasibility}\")\n\n# -----------------------------\n# 8. THRESHOLD PRIOR\n# -----------------------------\nTHR_START, THR_END = 0.30, 0.45\nprint(\"\\n[LOCKED THRESHOLD PRIOR]\")\nprint(f\"Use sweep range: {THR_START:.2f} â€“ {THR_END:.2f}\")\n\n# -----------------------------\n# 9. FINAL MANIFEST (DOWNSTREAM READY)\n# -----------------------------\ndf_manifest = df[[\n    \"image_path\",\n    \"mask_path\",\n    \"has_pothole\",\n    \"area_ratio\",\n    \"bucket\"\n]].copy()\n\nprint(f\"\\n[INFO] Final training samples: {len(df_manifest)}\")\n\nprint(\"\\n[STAGE 1 COMPLETE â€” LB-READY]\")\nprint(\"âœ“ Dataset validated\")\nprint(\"âœ“ Sampling buckets defined\")\nprint(\"âœ“ Min-area & threshold locked\")\nprint(\"âœ“ Manifest ready for STAGE 2/3\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T09:10:09.439291Z","iopub.execute_input":"2026-02-08T09:10:09.439556Z","iopub.status.idle":"2026-02-08T09:10:22.878821Z","shell.execute_reply.started":"2026-02-08T09:10:09.439519Z","shell.execute_reply":"2026-02-08T09:10:22.878055Z"},"papermill":{"duration":13.015336,"end_time":"2026-02-05T20:07:58.236972","exception":false,"start_time":"2026-02-05T20:07:45.221636","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[INFO] Train images : 498\n[INFO] Train masks  : 498\n[INFO] Test images  : 295\n[INFO] Valid image-mask pairs: 498\n","output_type":"stream"},{"name":"stderr","text":"Analyzing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498/498 [00:11<00:00, 41.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n[INSIGHT] Pothole presence:\nhas_pothole\n1    498\nName: count, dtype: int64\n\n[INSIGHT] Area ratio (% image):\ncount    498.000000\nmean       0.134860\nstd        0.128772\nmin        0.000235\n10%        0.007938\n25%        0.040943\n50%        0.091678\n75%        0.193834\n90%        0.329536\nmax        0.674005\nName: area_ratio, dtype: float64\n\n[INSIGHT] Bucket distribution:\nbucket\nlarge     0.691\nmedium    0.191\nsmall     0.066\ntiny      0.052\nName: proportion, dtype: float64\n\n[INSIGHT] Connected component area (px):\ncount    2.122000e+03\nmean     5.588544e+04\nstd      3.030841e+05\nmin      1.000000e+00\n10%      1.301000e+02\n25%      3.930000e+02\n50%      1.913000e+03\n75%      1.203275e+04\n90%      5.370160e+04\nmax      6.700584e+06\ndtype: float64\n\n[LOCKED PRIOR] MIN_AREA_PX â‰ˆ 130\n\n[FEASIBILITY] <1% area images: 11.85%\n[FEASIBILITY STATUS] FAVORABLE\n\n[LOCKED THRESHOLD PRIOR]\nUse sweep range: 0.30 â€“ 0.45\n\n[INFO] Final training samples: 498\n\n[STAGE 1 COMPLETE â€” LB-READY]\nâœ“ Dataset validated\nâœ“ Sampling buckets defined\nâœ“ Min-area & threshold locked\nâœ“ Manifest ready for STAGE 2/3\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"id":"7c3367d1","cell_type":"markdown","source":"# Preprocessing & Data Augmentation","metadata":{"papermill":{"duration":0.005254,"end_time":"2026-02-05T20:07:58.247873","exception":false,"start_time":"2026-02-05T20:07:58.242619","status":"completed"},"tags":[]}},{"id":"af03a0a6","cell_type":"code","source":"# ============================================================\n# STAGE 2 â€” Preprocessing & Data Augmentation (REVISED Â· ONE CELL)\n# TARGET: PUSH PUBLIC SCORE â†’ 0.80+\n# Philosophy:\n# - Dice-safe (NO mask destruction)\n# - Small & fragmented pothole recall priority\n# - SINGLE resolution (512) â€” train = val = test\n# - Mask-aware focus without overfitting\n# ============================================================\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\n# -----------------------------\n# NORMALIZATION (CONSISTENT)\n# -----------------------------\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\n# ============================================================\n# TRAIN AUGMENTATION â€” 512 (LB-SAFE)\n# ============================================================\ntrain_transform_512 = A.Compose(\n    [\n        # ---------------- Resolution ----------------\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n\n        # ---------------- Geometry (LOW-RISK) ----------------\n        A.HorizontalFlip(p=0.5),\n\n        A.Affine(\n            scale=(0.97, 1.05),\n            translate_percent=(0.0, 0.03),\n            rotate=(-2.5, 2.5),\n            shear=(-1.5, 1.5),\n            interpolation=cv2.INTER_LINEAR,\n            mode=cv2.BORDER_REFLECT_101,\n            p=0.35,  # â†“ slightly safer\n        ),\n\n        # ---------------- Mask-aware focus ----------------\n        # Encourage pothole-centric crops WITHOUT forcing it\n        A.RandomCrop(\n            height=448,\n            width=448,\n            p=0.30,\n        ),\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n\n        # ---------------- Photometric (PRIMARY DRIVER) ----------------\n        A.RandomBrightnessContrast(\n            brightness_limit=0.18,\n            contrast_limit=0.18,\n            p=0.65,\n        ),\n\n        A.HueSaturationValue(\n            hue_shift_limit=5,\n            sat_shift_limit=10,\n            val_shift_limit=5,\n            p=0.30,\n        ),\n\n        # ---------------- Shadow (CONSERVATIVE) ----------------\n        A.RandomShadow(\n            shadow_roi=(0, 0.6, 1, 1),\n            num_shadows_lower=1,\n            num_shadows_upper=1,\n            shadow_dimension=4,\n            p=0.18,  # â†“ safer for tiny pothole\n        ),\n\n        # ---------------- Texture Noise (VERY MILD) ----------------\n        A.OneOf(\n            [\n                A.GaussianBlur(blur_limit=3),\n                A.GaussNoise(var_limit=(4.0, 12.0)),\n            ],\n            p=0.15,\n        ),\n\n        # ---------------- Normalize ----------------\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n\n# ============================================================\n# VALIDATION TRANSFORM (STRICT)\n# ============================================================\nvalid_transform = A.Compose(\n    [\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n\n# ============================================================\n# TEST TRANSFORM (IDENTICAL TO VAL)\n# ============================================================\ntest_transform = A.Compose(\n    [\n        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ]\n)\n\n# ============================================================\n# FINAL CHECK\n# ============================================================\nprint(\"[STAGE 2 COMPLETE â€” LB-SAFE & 0.80-ORIENTED]\")\nprint(\"âœ“ Fixed resolution (512) â€” no mismatch\")\nprint(\"âœ“ Mask-aware spatial focus (no hard crop)\")\nprint(\"âœ“ Conservative geometry & shadow\")\nprint(\"âœ“ Small pothole recall preserved\")\nprint(\"âœ“ Fully compatible with STAGE 3+\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T09:10:22.880184Z","iopub.execute_input":"2026-02-08T09:10:22.880474Z","iopub.status.idle":"2026-02-08T09:10:27.639957Z","shell.execute_reply.started":"2026-02-08T09:10:22.880450Z","shell.execute_reply":"2026-02-08T09:10:27.639128Z"},"papermill":{"duration":6.054524,"end_time":"2026-02-05T20:08:04.307604","exception":false,"start_time":"2026-02-05T20:07:58.253080","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[STAGE 2 COMPLETE â€” LB-SAFE & 0.80-ORIENTED]\nâœ“ Fixed resolution (512) â€” no mismatch\nâœ“ Mask-aware spatial focus (no hard crop)\nâœ“ Conservative geometry & shadow\nâœ“ Small pothole recall preserved\nâœ“ Fully compatible with STAGE 3+\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3234331308.py:32: UserWarning: Argument(s) 'mode' are not valid for transform Affine\n  A.Affine(\n/tmp/ipykernel_55/3234331308.py:66: UserWarning: Argument(s) 'num_shadows_lower, num_shadows_upper' are not valid for transform RandomShadow\n  A.RandomShadow(\n/tmp/ipykernel_55/3234331308.py:78: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(4.0, 12.0)),\n","output_type":"stream"}],"execution_count":2},{"id":"9580fd61","cell_type":"markdown","source":"# Model Construction & Training","metadata":{"papermill":{"duration":0.005001,"end_time":"2026-02-05T20:08:04.317894","exception":false,"start_time":"2026-02-05T20:08:04.312893","status":"completed"},"tags":[]}},{"id":"22316092","cell_type":"code","source":"!pip install -q segmentation-models-pytorch==0.3.3 timm","metadata":{"execution":{"iopub.status.busy":"2026-02-08T09:10:27.640996Z","iopub.execute_input":"2026-02-08T09:10:27.641301Z","iopub.status.idle":"2026-02-08T09:10:37.856374Z","shell.execute_reply.started":"2026-02-08T09:10:27.641276Z","shell.execute_reply":"2026-02-08T09:10:37.855704Z"},"papermill":{"duration":10.935761,"end_time":"2026-02-05T20:08:15.258582","exception":false,"start_time":"2026-02-05T20:08:04.322821","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":3},{"id":"3757b921","cell_type":"code","source":"# ============================================================\n# STAGE 3 â€” PATCH + FULL IMAGE TRAINING (ONE CELL Â· FIXED)\n# - UNet++ EffNet-B4\n# - Patch-based foreground forcing\n# - Full-image finetune (512)\n# - SELF-CONTAINED (no external variables)\n# ============================================================\n\nimport random, re, cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nSEED = 42\nPATCH = 320\n\nPATCH_EPOCHS = 25\nFULL_EPOCHS  = 15\n\nPATCH_BATCH = 6\nFULL_BATCH  = 4\n\nLR_PATCH = 3e-4\nLR_FULL  = 1e-5\n\nPOS_WEIGHT = 8.0\nFREEZE_EPOCHS = 5\n\nIMG_SIZE = 512\n\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nIMG_DIR = DATA_ROOT / \"train/images\"\nMSK_DIR = DATA_ROOT / \"train/mask\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nprint(\"Device:\", DEVICE)\n\n# ============================================================\n# BUILD MANIFEST (SELF-CONTAINED)\n# ============================================================\ndef extract_idx(name):\n    return re.search(r\"(\\d+)\", name).group(1)\n\npairs = []\nfor img in IMG_DIR.iterdir():\n    idx = extract_idx(img.name)\n    msk = MSK_DIR / f\"mask_{idx}.png\"\n    if msk.exists():\n        pairs.append((str(img), str(msk)))\n\ndf = pd.DataFrame(pairs, columns=[\"image_path\", \"mask_path\"])\nprint(\"Total samples:\", len(df))\n\ndf_train, df_val = train_test_split(\n    df, test_size=0.15, random_state=SEED, shuffle=True\n)\nprint(\"Train:\", len(df_train), \"| Val:\", len(df_val))\n\n# ============================================================\n# SAFE PATCH DATASET (NO SIZE MISMATCH)\n# ============================================================\n\nclass PatchDataset(Dataset):\n    def __init__(self, df, transform, patch=320):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.patch = patch\n\n    def __len__(self):\n        return len(self.df) * 3\n\n    def _safe_crop(self, img, mask, x1, y1):\n        h, w = img.shape[:2]\n        x1 = max(0, min(x1, w - self.patch))\n        y1 = max(0, min(y1, h - self.patch))\n\n        img_c = img[y1:y1+self.patch, x1:x1+self.patch]\n        mask_c = mask[y1:y1+self.patch, x1:x1+self.patch]\n\n        # ðŸ”’ HARD GUARD\n        if img_c.shape[0] != self.patch or img_c.shape[1] != self.patch:\n            img_c = cv2.resize(img, (self.patch, self.patch))\n            mask_c = cv2.resize(mask, (self.patch, self.patch), interpolation=cv2.INTER_NEAREST)\n\n        return img_c, mask_c\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx % len(self.df)]\n        img = cv2.imread(row[\"image_path\"])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(row[\"mask_path\"], cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(np.uint8)\n\n        h, w = img.shape[:2]\n\n        # ---------- FOREGROUND PATCH ----------\n        if mask.sum() > 0 and random.random() < 0.7:\n            ys, xs = np.where(mask > 0)\n            i = random.randint(0, len(xs) - 1)\n            cx, cy = xs[i], ys[i]\n            x1 = cx - self.patch // 2\n            y1 = cy - self.patch // 2\n            img_c, mask_c = self._safe_crop(img, mask, x1, y1)\n\n        # ---------- RANDOM PATCH ----------\n        else:\n            if h >= self.patch and w >= self.patch:\n                x1 = random.randint(0, w - self.patch)\n                y1 = random.randint(0, h - self.patch)\n                img_c, mask_c = self._safe_crop(img, mask, x1, y1)\n            else:\n                # fallback resize\n                img_c = cv2.resize(img, (self.patch, self.patch))\n                mask_c = cv2.resize(mask, (self.patch, self.patch), interpolation=cv2.INTER_NEAREST)\n\n        aug = self.transform(image=img_c, mask=mask_c)\n        return aug[\"image\"], aug[\"mask\"].unsqueeze(0).float()\n\n\n# ============================================================\n# TRANSFORMS\n# ============================================================\npatch_tf = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(0.15, 0.15, p=0.5),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(),\n])\n\nfull_train_tf = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(),\n])\n\nfull_val_tf = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2(),\n])\n\n# ============================================================\n# MODEL\n# ============================================================\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n).to(DEVICE)\n\ndice = smp.losses.DiceLoss(mode=\"binary\", from_logits=True)\nbce  = torch.nn.BCEWithLogitsLoss(\n    pos_weight=torch.tensor(POS_WEIGHT, device=DEVICE)\n)\n\n# ============================================================\n# PHASE A â€” PATCH TRAINING\n# ============================================================\nprint(\"\\n[PHASE A] Patch-based training\")\n\npatch_loader = DataLoader(\n    PatchDataset(df_train, patch_tf),\n    batch_size=PATCH_BATCH,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n)\n\noptimizer = AdamW(model.parameters(), lr=LR_PATCH, weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=PATCH_EPOCHS)\n\nmodel.train()\nfor epoch in range(1, PATCH_EPOCHS + 1):\n    total = 0\n    for imgs, masks in tqdm(patch_loader, desc=f\"Patch {epoch}\"):\n        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = 0.5 * dice(logits, masks) + 0.5 * bce(logits, masks)\n        loss.backward()\n        optimizer.step()\n        total += loss.item()\n    scheduler.step()\n    print(f\"Patch Epoch {epoch:02d} | Loss {total:.4f}\")\n\n# ============================================================\n# PHASE B â€” FULL IMAGE FINETUNE\n# ============================================================\nprint(\"\\n[PHASE B] Full-image finetuning\")\n\nfor p in model.encoder.parameters():\n    p.requires_grad = False\n\noptimizer = AdamW(\n    [\n        {\"params\": model.encoder.parameters(), \"lr\": LR_FULL * 0.1},\n        {\"params\": model.decoder.parameters(), \"lr\": LR_FULL},\n    ],\n    weight_decay=1e-4,\n)\n\nscheduler = CosineAnnealingLR(optimizer, T_max=FULL_EPOCHS)\n\ntrain_loader = DataLoader(\n    FullDataset(df_train, full_train_tf),\n    batch_size=FULL_BATCH,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n)\n\nval_loader = DataLoader(\n    FullDataset(df_val, full_val_tf),\n    batch_size=FULL_BATCH,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n)\n\nbest_val = 0.0\n\n@torch.no_grad()\ndef validate():\n    model.eval()\n    dices = []\n    for imgs, masks in val_loader:\n        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n        prob = torch.sigmoid(model(imgs))\n        pred = (prob > 0.4).float()\n        inter = (pred * masks).sum((2,3))\n        union = pred.sum((2,3)) + masks.sum((2,3))\n        dices.append(((2 * inter + 1e-7) / (union + 1e-7)).mean().item())\n    return float(np.mean(dices))\n\nfor epoch in range(1, FULL_EPOCHS + 1):\n    model.train()\n\n    if epoch == FREEZE_EPOCHS + 1:\n        for p in model.encoder.parameters():\n            p.requires_grad = True\n        print(\"[INFO] Encoder unfrozen\")\n\n    for imgs, masks in tqdm(train_loader, desc=f\"Full {epoch}\"):\n        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = dice(logits, masks) + 0.5 * bce(logits, masks)\n        loss.backward()\n        optimizer.step()\n\n    scheduler.step()\n    val_dice = validate()\n    print(f\"Full Epoch {epoch:02d} | ValDice {val_dice:.4f}\")\n\n    if val_dice > best_val:\n        best_val = val_dice\n        torch.save(model.state_dict(), \"/kaggle/working/unetpp_best.pt\")\n        print(\">> Best model saved\")\n\nprint(f\"\\n[BEST VALIDATION DICE] {best_val:.4f}\")\nprint(\"[STAGE 3 COMPLETE â€” PATCH + FULL TRAINING DONE]\")","metadata":{"execution":{"iopub.status.busy":"2026-02-08T09:29:28.991137Z","iopub.execute_input":"2026-02-08T09:29:28.992121Z","iopub.status.idle":"2026-02-08T10:23:41.331164Z","shell.execute_reply.started":"2026-02-08T09:29:28.992049Z","shell.execute_reply":"2026-02-08T10:23:41.330261Z"},"papermill":{"duration":3531.638821,"end_time":"2026-02-05T21:07:06.903369","exception":false,"start_time":"2026-02-05T20:08:15.264548","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Device: cuda\nTotal samples: 498\nTrain: 423 | Val: 75\n\n[PHASE A] Patch-based training\n","output_type":"stream"},{"name":"stderr","text":"Patch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:30<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 01 | Loss 168.8204\n","output_type":"stream"},{"name":"stderr","text":"Patch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 02 | Loss 133.2683\n","output_type":"stream"},{"name":"stderr","text":"Patch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 03 | Loss 112.8592\n","output_type":"stream"},{"name":"stderr","text":"Patch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 04 | Loss 109.2608\n","output_type":"stream"},{"name":"stderr","text":"Patch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 05 | Loss 100.2822\n","output_type":"stream"},{"name":"stderr","text":"Patch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 06 | Loss 93.9750\n","output_type":"stream"},{"name":"stderr","text":"Patch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 07 | Loss 89.5055\n","output_type":"stream"},{"name":"stderr","text":"Patch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 08 | Loss 82.0432\n","output_type":"stream"},{"name":"stderr","text":"Patch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 09 | Loss 79.7558\n","output_type":"stream"},{"name":"stderr","text":"Patch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 10 | Loss 77.7587\n","output_type":"stream"},{"name":"stderr","text":"Patch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 11 | Loss 73.5017\n","output_type":"stream"},{"name":"stderr","text":"Patch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 12 | Loss 72.2080\n","output_type":"stream"},{"name":"stderr","text":"Patch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 13 | Loss 69.2927\n","output_type":"stream"},{"name":"stderr","text":"Patch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 14 | Loss 68.6769\n","output_type":"stream"},{"name":"stderr","text":"Patch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:30<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 15 | Loss 64.8873\n","output_type":"stream"},{"name":"stderr","text":"Patch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 16 | Loss 60.2991\n","output_type":"stream"},{"name":"stderr","text":"Patch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 17 | Loss 58.1655\n","output_type":"stream"},{"name":"stderr","text":"Patch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 18 | Loss 52.8673\n","output_type":"stream"},{"name":"stderr","text":"Patch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:30<00:00,  2.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 19 | Loss 53.2322\n","output_type":"stream"},{"name":"stderr","text":"Patch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 20 | Loss 53.7305\n","output_type":"stream"},{"name":"stderr","text":"Patch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 21 | Loss 49.8642\n","output_type":"stream"},{"name":"stderr","text":"Patch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 22 | Loss 50.6739\n","output_type":"stream"},{"name":"stderr","text":"Patch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:30<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 23 | Loss 49.0583\n","output_type":"stream"},{"name":"stderr","text":"Patch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 24 | Loss 48.2337\n","output_type":"stream"},{"name":"stderr","text":"Patch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212/212 [01:29<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Patch Epoch 25 | Loss 45.1070\n\n[PHASE B] Full-image finetuning\n","output_type":"stream"},{"name":"stderr","text":"Full 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:42<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 01 | ValDice 0.6805\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:42<00:00,  2.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 02 | ValDice 0.6831\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:43<00:00,  2.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 03 | ValDice 0.6804\n","output_type":"stream"},{"name":"stderr","text":"Full 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:42<00:00,  2.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 04 | ValDice 0.6848\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [00:42<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 05 | ValDice 0.6855\n>> Best model saved\n[INFO] Encoder unfrozen\n","output_type":"stream"},{"name":"stderr","text":"Full 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 06 | ValDice 0.6843\n","output_type":"stream"},{"name":"stderr","text":"Full 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 07 | ValDice 0.6903\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 08 | ValDice 0.6868\n","output_type":"stream"},{"name":"stderr","text":"Full 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 09 | ValDice 0.6894\n","output_type":"stream"},{"name":"stderr","text":"Full 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 10 | ValDice 0.6873\n","output_type":"stream"},{"name":"stderr","text":"Full 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 11 | ValDice 0.6910\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 12 | ValDice 0.6932\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 13 | ValDice 0.6936\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 14 | ValDice 0.6953\n>> Best model saved\n","output_type":"stream"},{"name":"stderr","text":"Full 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106/106 [01:13<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Full Epoch 15 | ValDice 0.6941\n\n[BEST VALIDATION DICE] 0.6953\n[STAGE 3 COMPLETE â€” PATCH + FULL TRAINING DONE]\n","output_type":"stream"}],"execution_count":6},{"id":"b57bfc06","cell_type":"markdown","source":"# Optimization, Validation & Refinement","metadata":{"papermill":{"duration":0.188839,"end_time":"2026-02-05T21:07:07.280193","exception":false,"start_time":"2026-02-05T21:07:07.091354","status":"completed"},"tags":[]}},{"id":"bd7e3434","cell_type":"code","source":"# ============================================================\n# STAGE 4 â€” Optimization & Refinement (FINAL Â· LB-SAFE)\n# - Same validation set as STAGE 3\n# - UNet++ ONLY\n# - Patch-aware threshold range\n# - Strict Dice (empty-safe)\n# ============================================================\n\n!pip install -q optuna\n\nimport optuna\nimport numpy as np\nimport torch\nimport cv2\nfrom tqdm import tqdm\n\nimport segmentation_models_pytorch as smp\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# -----------------------------\n# DEVICE\n# -----------------------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\n# ============================================================\n# VALIDATION SET (FROM STAGE 3)\n# ============================================================\ndf_val = df_val.copy()\n\n# ============================================================\n# DATASET\n# ============================================================\nclass PotholeDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.df.loc[idx, \"image_path\"])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.df.loc[idx, \"mask_path\"], cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(\"uint8\")\n        aug = self.transform(image=img, mask=mask)\n        return aug[\"image\"], aug[\"mask\"]\n\n# ============================================================\n# TRANSFORM (IDENTICAL TO STAGE 3 VAL)\n# ============================================================\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\nvalid_tf = A.Compose([\n    A.Resize(512, 512),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nval_loader = DataLoader(\n    PotholeDataset(df_val, valid_tf),\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n)\n\n# ============================================================\n# LOAD MODEL\n# ============================================================\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=None,\n    in_channels=3,\n    classes=1,\n).to(DEVICE)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/unetpp_best.pt\", map_location=DEVICE))\nmodel.eval()\n\nprint(\"[INFO] UNet++ loaded\")\n\n# ============================================================\n# COMPUTE MIN_AREA RANGE (DATA-DRIVEN)\n# ============================================================\nareas = []\nfor _, row in df_val.iterrows():\n    m = cv2.imread(row[\"mask_path\"], cv2.IMREAD_GRAYSCALE)\n    if m is None:\n        continue\n    m = (m == 255).astype(np.uint8)\n    n, _, stats, _ = cv2.connectedComponentsWithStats(m, connectivity=8)\n    for i in range(1, n):\n        areas.append(stats[i, cv2.CC_STAT_AREA])\n\nareas = np.array(areas)\nMIN_AREA_LO = int(np.percentile(areas, 10))\nMIN_AREA_HI = int(np.percentile(areas, 50))\n\nprint(f\"[INFO] min_area search range: {MIN_AREA_LO} â€“ {MIN_AREA_HI}\")\n\n# ============================================================\n# METRIC (STRICT DICE)\n# ============================================================\ndef dice_strict(pred, gt, eps=1e-7):\n    if gt.sum() == 0:\n        return 0.0\n    inter = (pred * gt).sum()\n    union = pred.sum() + gt.sum()\n    return (2 * inter + eps) / (union + eps)\n\ndef remove_small(mask, min_area):\n    n, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    out = np.zeros_like(mask)\n    for i in range(1, n):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            out[labels == i] = 1\n    return out\n\n# ============================================================\n# OPTUNA OBJECTIVE\n# ============================================================\ndef objective(trial):\n    thr = trial.suggest_float(\"threshold\", 0.45, 0.65)\n    min_area = trial.suggest_int(\"min_area\", MIN_AREA_LO, MIN_AREA_HI, step=20)\n\n    scores = []\n\n    with torch.no_grad():\n        for imgs, masks in val_loader:\n            imgs = imgs.to(DEVICE)\n            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n            masks = masks.numpy()\n\n            for i in range(len(probs)):\n                p = (probs[i, 0] > thr).astype(np.uint8)\n                p = remove_small(p, min_area)\n                scores.append(dice_strict(p, masks[i]))\n\n    return float(np.mean(scores))\n\n# ============================================================\n# RUN OPTUNA\n# ============================================================\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=30, show_progress_bar=True)\n\nbest = study.best_params\n\nprint(\"\\n[OPTUNA BEST CONFIG â€” FINAL]\")\nfor k, v in best.items():\n    print(f\"{k}: {v}\")\nprint(f\"Validation Dice: {study.best_value:.4f}\")\n\n# ============================================================\n# EXPORT CONFIG\n# ============================================================\nOPT_CONFIG = {\n    \"weights\": {\"unetpp\": 1.0},\n    \"threshold\": best[\"threshold\"],\n    \"min_area\": best[\"min_area\"],\n}\n\nprint(\"\\n[STAGE 4 COMPLETE â€” PATCH-AWARE & LB-SAFE]\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T10:23:41.333125Z","iopub.status.idle":"2026-02-08T10:25:49.239981Z","shell.execute_reply.started":"2026-02-08T10:23:41.333434Z","shell.execute_reply":"2026-02-08T10:25:49.239195Z"},"papermill":{"duration":303.869045,"end_time":"2026-02-05T21:12:11.335789","exception":false,"start_time":"2026-02-05T21:07:07.466744","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[I 2026-02-08 10:25:16,429] Trial 21 finished with value: 0.6898246257009887 and parameters: {'threshold': 0.5089192109660496, 'min_area': 158}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:20,523] Trial 22 finished with value: 0.6827578776190841 and parameters: {'threshold': 0.5292345817245022, 'min_area': 358}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:24,603] Trial 23 finished with value: 0.6959317165298039 and parameters: {'threshold': 0.5007252879547636, 'min_area': 118}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:28,690] Trial 24 finished with value: 0.6817422896944326 and parameters: {'threshold': 0.5876905302182233, 'min_area': 358}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:32,808] Trial 25 finished with value: 0.6691485460556624 and parameters: {'threshold': 0.45315039663058126, 'min_area': 598}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:36,910] Trial 26 finished with value: 0.6702907355266757 and parameters: {'threshold': 0.5603217840433132, 'min_area': 818}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:41,061] Trial 27 finished with value: 0.6610561059335394 and parameters: {'threshold': 0.510842034220507, 'min_area': 1578}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:45,162] Trial 28 finished with value: 0.6840204449192255 and parameters: {'threshold': 0.5434054645934835, 'min_area': 298}. Best is trial 12 with value: 0.7017548544844897.\n[I 2026-02-08 10:25:49,233] Trial 29 finished with value: 0.66543350707116 and parameters: {'threshold': 0.6266120579063569, 'min_area': 1838}. Best is trial 12 with value: 0.7017548544844897.\n\n[OPTUNA BEST CONFIG â€” FINAL]\nthreshold: 0.5630802287849173\nmin_area: 118\nValidation Dice: 0.7018\n\n[STAGE 4 COMPLETE â€” PATCH-AWARE & LB-SAFE]\n","output_type":"stream"}],"execution_count":7},{"id":"540bbf7a","cell_type":"markdown","source":"# Inference, Encoding & Submission","metadata":{"papermill":{"duration":0.18696,"end_time":"2026-02-05T21:12:11.715168","exception":false,"start_time":"2026-02-05T21:12:11.528208","status":"completed"},"tags":[]}},{"id":"371df322","cell_type":"code","source":"# ============================================================\n# STAGE 5 â€” FINAL INFERENCE, RLE & SUBMISSION (LB-SAFE)\n# - All postprocess done in 512-space\n# - Threshold & min_area consistent with Stage 4\n# - Pixel-count empty guard (NOT mean-prob)\n# - H-Flip TTA\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport cv2\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTEST_IMG_DIR = DATA_ROOT / \"test/images\"\nSAMPLE_SUB = Path(\"/kaggle/input/data-science-ara-7-0/sample_submission.csv\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTHRESHOLD = OPT_CONFIG[\"threshold\"]\nMIN_AREA_512 = OPT_CONFIG[\"min_area\"]\n\nINPUT_SIZE = 512\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\n# -----------------------------\n# LOAD MODEL\n# -----------------------------\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=None,\n    in_channels=3,\n    classes=1,\n).to(DEVICE)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/unetpp_best.pt\", map_location=DEVICE))\nmodel.eval()\n\nprint(\"[INFO] UNet++ loaded\")\n\n# -----------------------------\n# TRANSFORM (512 ONLY)\n# -----------------------------\ninfer_tf = A.Compose([\n    A.Resize(INPUT_SIZE, INPUT_SIZE),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\n# -----------------------------\n# RLE ENCODER (OFFICIAL)\n# -----------------------------\ndef encode_rle(mask: np.ndarray) -> str:\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[0::2]\n    return \" \".join(map(str, runs))\n\n# -----------------------------\n# POSTPROCESS (512-SPACE)\n# -----------------------------\ndef remove_small(mask, min_area):\n    n, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    out = np.zeros_like(mask, dtype=np.uint8)\n    for i in range(1, n):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            out[labels == i] = 1\n    return out\n\n# -----------------------------\n# INFERENCE\n# -----------------------------\nrecords = []\ntest_images = sorted(TEST_IMG_DIR.glob(\"*.jpg\"))\n\nwith torch.no_grad():\n    for img_path in tqdm(test_images, desc=\"Final Inference\"):\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h0, w0 = img.shape[:2]\n\n        x = infer_tf(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n        x_f = torch.flip(x, dims=[3])\n\n        # --- forward + TTA ---\n        p  = torch.sigmoid(model(x))\n        p_f = torch.flip(torch.sigmoid(model(x_f)), dims=[3])\n        prob = ((p + p_f) / 2.0)[0, 0].cpu().numpy()   # (512,512)\n\n        # --- threshold in 512 space ---\n        pred_512 = (prob > THRESHOLD).astype(np.uint8)\n\n        # --- remove small objects (512 space) ---\n        pred_512 = remove_small(pred_512, MIN_AREA_512)\n\n        # --- pixel-count empty guard ---\n        if pred_512.sum() < MIN_AREA_512:\n            pred_512[:] = 0\n\n        # --- resize mask to original ---\n        pred = cv2.resize(\n            pred_512, (w0, h0), interpolation=cv2.INTER_NEAREST\n        )\n\n        rle = \"\" if pred.sum() == 0 else encode_rle(pred)\n\n        records.append({\n            \"ImageId\": img_path.name,\n            \"rle\": rle\n        })\n\n# -----------------------------\n# SUBMISSION\n# -----------------------------\ndf_sub = pd.DataFrame(records)\ndf_sample = pd.read_csv(SAMPLE_SUB)\ndf_sub = df_sub[df_sample.columns.tolist()]\n\nOUT_SUB = \"/kaggle/working/submission.csv\"\ndf_sub.to_csv(OUT_SUB, index=False)\n\nprint(\"\\n[STAGE 5 COMPLETE â€” FINAL LB-SAFE SUBMISSION READY]\")\nprint(\"Saved to:\", OUT_SUB)\nprint(\"Total:\", len(df_sub))\nprint(\"Empty RLE:\", (df_sub['rle'] == '').sum())\nprint(df_sub.head())\n","metadata":{"execution":{"iopub.status.busy":"2026-02-08T10:25:49.241215Z","iopub.execute_input":"2026-02-08T10:25:49.241455Z","iopub.status.idle":"2026-02-08T10:26:23.464729Z","shell.execute_reply.started":"2026-02-08T10:25:49.241427Z","shell.execute_reply":"2026-02-08T10:26:23.463986Z"},"papermill":{"duration":62.681733,"end_time":"2026-02-05T21:13:14.583725","exception":false,"start_time":"2026-02-05T21:12:11.901992","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[INFO] UNet++ loaded\n","output_type":"stream"},{"name":"stderr","text":"Final Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:33<00:00,  8.76it/s]","output_type":"stream"},{"name":"stdout","text":"\n[STAGE 5 COMPLETE â€” FINAL LB-SAFE SUBMISSION READY]\nSaved to: /kaggle/working/submission.csv\nTotal: 295\nEmpty RLE: 0\n        ImageId                                                rle\n0  test_001.jpg  4342 4 4641 5 4940 7 5239 8 5539 9 5837 12 613...\n1  test_002.jpg  61612 2 62125 5 62328 8 62845 5 63048 8 63564 ...\n2  test_003.jpg  624509 4 626805 4 629101 4 631397 4 633693 4 6...\n3  test_004.jpg  1010 3 1310 3 1609 4 1909 4 2209 4 2509 4 2809...\n4  test_005.jpg  50217 1 50223 3 50518 19 50813 26 51113 27 514...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8}]}