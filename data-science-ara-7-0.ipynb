{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":128328,"databundleVersionId":15445689,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Understanding & Preparation","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — Data Understanding & Preparation (FINAL)\n# Fix:\n# - Handle filename mismatch: train_XXX ↔ mask_XXX\n# - Strict validation without false assertion\n# - Build clean training manifest\n# ============================================================\n\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\n\nTRAIN_IMG_DIR = DATA_ROOT / \"train\" / \"images\"\nTRAIN_MASK_DIR = DATA_ROOT / \"train\" / \"mask\"\nTEST_IMG_DIR  = DATA_ROOT / \"test\" / \"images\"\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n\n# -----------------------------\n# 1. LOAD FILES\n# -----------------------------\ntrain_images = sorted([p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntrain_masks  = sorted([p for p in TRAIN_MASK_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntest_images  = sorted([p for p in TEST_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n\nprint(f\"[INFO] Train images : {len(train_images)}\")\nprint(f\"[INFO] Train masks  : {len(train_masks)}\")\nprint(f\"[INFO] Test images  : {len(test_images)}\")\n\n# -----------------------------\n# 2. BUILD MASK INDEX BY NUMBER\n# -----------------------------\ndef extract_index(name: str):\n    \"\"\"\n    Extract numeric id from:\n    - train_048.jpg\n    - mask_048.png\n    \"\"\"\n    m = re.search(r\"(\\d+)\", name)\n    return m.group(1) if m else None\n\nmask_index = {}\nfor m in train_masks:\n    idx = extract_index(m.stem)\n    if idx is not None:\n        mask_index[idx] = m\n\n# -----------------------------\n# 3. PAIR IMAGE–MASK\n# -----------------------------\npairs = []\nmissing = []\n\nfor img in train_images:\n    idx = extract_index(img.stem)\n    if idx in mask_index:\n        pairs.append({\n            \"image_path\": img,\n            \"mask_path\": mask_index[idx],\n            \"id\": idx\n        })\n    else:\n        missing.append(img.name)\n\nprint(f\"[INFO] Valid pairs : {len(pairs)}\")\nprint(f\"[WARNING] Missing masks : {len(missing)}\")\n\nif missing:\n    print(\"[WARNING] Example missing:\", missing[:10])\n\nassert len(pairs) > 0, \"No valid image-mask pairs found\"\n\n# -----------------------------\n# 4. SANITY CHECK (SHAPE + VALUES)\n# -----------------------------\nrecords = []\n\nfor p in tqdm(pairs, desc=\"Validating pairs\"):\n    img = cv2.imread(str(p[\"image_path\"]))\n    mask = cv2.imread(str(p[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n\n    assert img is not None, f\"Failed to read image {p['image_path']}\"\n    assert mask is not None, f\"Failed to read mask {p['mask_path']}\"\n    assert img.shape[:2] == mask.shape, f\"Shape mismatch: {p['image_path'].name}\"\n\n    uniq = np.unique(mask)\n\n    records.append({\n        \"image\": p[\"image_path\"].name,\n        \"mask\": p[\"mask_path\"].name,\n        \"height\": mask.shape[0],\n        \"width\": mask.shape[1],\n        \"unique_values\": uniq.tolist(),\n        \"has_pothole\": int((mask == 255).any())\n    })\n\ndf_info = pd.DataFrame(records)\n\n# -----------------------------\n# 5. DATASET STATS\n# -----------------------------\nprint(\"\\n[INFO] Unique mask values:\")\nprint(sorted(set(v for row in df_info[\"unique_values\"] for v in row)))\n\nprint(\"\\n[INFO] Pothole presence:\")\nprint(df_info[\"has_pothole\"].value_counts())\n\nprint(\"\\n[INFO] Resolution distribution (top):\")\nprint(df_info.groupby([\"height\", \"width\"]).size().sort_values(ascending=False).head())\n\n# -----------------------------\n# 6. STRICT VALIDATION\n# -----------------------------\nall_vals = set(v for row in df_info[\"unique_values\"] for v in row)\nassert all_vals.issubset({0, 255}), f\"Invalid mask values detected: {all_vals}\"\n\n# -----------------------------\n# 7. EXPORT CLEAN MANIFEST\n# -----------------------------\ndf_manifest = pd.DataFrame({\n    \"image_path\": [str(p[\"image_path\"]) for p in pairs],\n    \"mask_path\":  [str(p[\"mask_path\"]) for p in pairs],\n    \"id\":         [p[\"id\"] for p in pairs],\n})\n\nprint(f\"\\n[INFO] Final usable training samples: {len(df_manifest)}\")\n\n# Optional save\n# df_manifest.to_csv(\"/kaggle/working/train_manifest.csv\", index=False)\n\nprint(\"\\n[STAGE 1 COMPLETE]\")\nprint(\"Image–mask mapping correct (train_xxx ↔ mask_xxx)\")\nprint(\"Dataset fully validated\")\nprint(\"Ready for STAGE 2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:36:01.946502Z","iopub.execute_input":"2026-02-05T00:36:01.947575Z","iopub.status.idle":"2026-02-05T00:37:01.415763Z","shell.execute_reply.started":"2026-02-05T00:36:01.947532Z","shell.execute_reply":"2026-02-05T00:37:01.414873Z"}},"outputs":[{"name":"stdout","text":"[INFO] Train images : 498\n[INFO] Train masks  : 498\n[INFO] Test images  : 295\n[INFO] Valid pairs : 498\n[WARNING] Missing masks : 0\n","output_type":"stream"},{"name":"stderr","text":"Validating pairs: 100%|██████████| 498/498 [00:59<00:00,  8.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n[INFO] Unique mask values:\n[0, 255]\n\n[INFO] Pothole presence:\nhas_pothole\n1    498\nName: count, dtype: int64\n\n[INFO] Resolution distribution (top):\nheight  width\n720     720      74\n4160    3120     50\n2760    3680     39\n234     416      29\n360     640      27\ndtype: int64\n\n[INFO] Final usable training samples: 498\n\n[STAGE 1 COMPLETE]\nImage–mask mapping correct (train_xxx ↔ mask_xxx)\nDataset fully validated\nReady for STAGE 2\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Preprocessing & Data Augmentation","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — Preprocessing & Data Augmentation\n# Purpose:\n# - Define resize strategy (fixed-size)\n# - Normalize input for EfficientNet encoder\n# - Build train / valid / test augmentation pipelines\n# Notes:\n# - Mask kept binary {0,1} for training\n# - Conversion to {0,255} happens only at submission stage\n# ============================================================\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np\n\n# -----------------------------\n# GLOBAL SETTINGS\n# -----------------------------\n# Recommended: 512 (safe) or 640 (higher PB if GPU allows)\nINPUT_SIZE = 512\n\n# ImageNet normalization (required for EfficientNet)\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\n# -----------------------------\n# 1. TRAIN AUGMENTATION\n# -----------------------------\ntrain_transform = A.Compose(\n    [\n        # --- geometric ---\n        A.Resize(INPUT_SIZE, INPUT_SIZE, interpolation=cv2.INTER_LINEAR),\n        A.HorizontalFlip(p=0.5),\n\n        # --- photometric (road-specific) ---\n        A.RandomBrightnessContrast(\n            brightness_limit=0.2,\n            contrast_limit=0.2,\n            p=0.7\n        ),\n        A.HueSaturationValue(\n            hue_shift_limit=10,\n            sat_shift_limit=15,\n            val_shift_limit=10,\n            p=0.5\n        ),\n        A.RandomShadow(\n            shadow_roi=(0, 0.5, 1, 1),\n            num_shadows_lower=1,\n            num_shadows_upper=2,\n            shadow_dimension=5,\n            p=0.3\n        ),\n\n        # --- texture noise ---\n        A.GaussianBlur(blur_limit=3, p=0.2),\n\n        # --- normalize & tensor ---\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n\n# -----------------------------\n# 2. VALIDATION AUGMENTATION\n# (NO randomness)\n# -----------------------------\nvalid_transform = A.Compose(\n    [\n        A.Resize(INPUT_SIZE, INPUT_SIZE, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n\n# -----------------------------\n# 3. TEST AUGMENTATION\n# (image only)\n# -----------------------------\ntest_transform = A.Compose(\n    [\n        A.Resize(INPUT_SIZE, INPUT_SIZE, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ]\n)\n\n# -----------------------------\n# 4. QUICK SANITY PRINT\n# -----------------------------\nprint(\"[STAGE 2 READY]\")\nprint(f\"Input size          : {INPUT_SIZE} x {INPUT_SIZE}\")\nprint(\"Normalization       : ImageNet (EfficientNet compatible)\")\nprint(\"Train augmentation  : ON (lighting, shadow, blur, flip)\")\nprint(\"Validation/test aug : OFF (deterministic)\")\nprint(\"Safe to proceed to STAGE 3 (Dataset & DataLoader)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:38:20.515863Z","iopub.execute_input":"2026-02-05T00:38:20.516833Z","iopub.status.idle":"2026-02-05T00:38:27.514157Z","shell.execute_reply.started":"2026-02-05T00:38:20.516800Z","shell.execute_reply":"2026-02-05T00:38:27.512889Z"}},"outputs":[{"name":"stdout","text":"[STAGE 2 READY]\nInput size          : 512 x 512\nNormalization       : ImageNet (EfficientNet compatible)\nTrain augmentation  : ON (lighting, shadow, blur, flip)\nValidation/test aug : OFF (deterministic)\nSafe to proceed to STAGE 3 (Dataset & DataLoader)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/411835882.py:48: UserWarning: Argument(s) 'num_shadows_lower, num_shadows_upper' are not valid for transform RandomShadow\n  A.RandomShadow(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Model Construction & Training","metadata":{}},{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch==0.3.3 timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:40:44.011088Z","iopub.execute_input":"2026-02-05T00:40:44.011464Z","iopub.status.idle":"2026-02-05T00:40:58.073216Z","shell.execute_reply.started":"2026-02-05T00:40:44.011435Z","shell.execute_reply":"2026-02-05T00:40:58.072240Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — Model Construction & Training (REVISED)\n# Fix:\n# - Install segmentation-models-pytorch inside cell\n# ============================================================\n\n# 1. IMPORTS\n# -----------------------------\nimport os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nimport segmentation_models_pytorch as smp\n\n# -----------------------------\n# 2. REPRODUCIBILITY\n# -----------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# -----------------------------\n# 3. LOAD DATA (FROM STAGE 1 LOGIC)\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTRAIN_IMG_DIR = DATA_ROOT / \"train\" / \"images\"\nTRAIN_MASK_DIR = DATA_ROOT / \"train\" / \"mask\"\n\nimport re\ndef extract_idx(name):\n    m = re.search(r\"(\\d+)\", name)\n    return m.group(1) if m else None\n\npairs = []\nfor img in TRAIN_IMG_DIR.iterdir():\n    idx = extract_idx(img.name)\n    mask = TRAIN_MASK_DIR / f\"mask_{idx}.png\"\n    if mask.exists():\n        pairs.append((str(img), str(mask)))\n\ndf = pd.DataFrame(pairs, columns=[\"image_path\", \"mask_path\"])\nprint(\"Total training samples:\", len(df))\n\n# -----------------------------\n# 4. TRAIN / VALID SPLIT\n# -----------------------------\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_valid = train_test_split(\n    df,\n    test_size=0.15,\n    random_state=SEED,\n    shuffle=True\n)\n\nprint(\"Train:\", len(df_train), \"Valid:\", len(df_valid))\n\n# -----------------------------\n# 5. DATASET CLASS\n# -----------------------------\nclass PotholeDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.df.loc[idx, \"image_path\"])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(self.df.loc[idx, \"mask_path\"], cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(\"float32\")\n\n        augmented = self.transform(image=img, mask=mask)\n        img = augmented[\"image\"]\n        mask = augmented[\"mask\"].unsqueeze(0)\n\n        return img, mask\n\n# -----------------------------\n# 6. DATALOADERS\n# -----------------------------\nBATCH_SIZE = 4\n\ntrain_ds = PotholeDataset(df_train, train_transform)\nvalid_ds = PotholeDataset(df_valid, valid_transform)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nvalid_loader = DataLoader(\n    valid_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n# -----------------------------\n# 7. MODEL\n# -----------------------------\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n)\n\nmodel.to(device)\n\n# -----------------------------\n# 8. LOSS\n# -----------------------------\ndice_loss = smp.losses.DiceLoss(mode=\"binary\")\nfocal_loss = smp.losses.FocalLoss(mode=\"binary\")\n\ndef criterion(pred, target):\n    return dice_loss(pred, target) + focal_loss(pred, target)\n\n# -----------------------------\n# 9. METRIC\n# -----------------------------\ndef dice_coef(pred, target, eps=1e-7):\n    pred = (pred > 0.5).float()\n    intersection = (pred * target).sum(dim=(2,3))\n    union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    dice = (2 * intersection + eps) / (union + eps)\n    return dice.mean()\n\n# -----------------------------\n# 10. OPTIMIZER\n# -----------------------------\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n# -----------------------------\n# 11. TRAINING LOOP\n# -----------------------------\nEPOCHS = 30\nbest_dice = 0.0\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    train_loss = 0.0\n\n    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch} [TRAIN]\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = criterion(logits, masks)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n\n    model.eval()\n    val_loss = 0.0\n    val_dice = 0.0\n\n    with torch.no_grad():\n        for imgs, masks in tqdm(valid_loader, desc=f\"Epoch {epoch} [VALID]\"):\n            imgs = imgs.to(device)\n            masks = masks.to(device)\n\n            logits = model(imgs)\n            loss = criterion(logits, masks)\n            dice = dice_coef(torch.sigmoid(logits), masks)\n\n            val_loss += loss.item()\n            val_dice += dice.item()\n\n    val_loss /= len(valid_loader)\n    val_dice /= len(valid_loader)\n\n    print(\n        f\"Epoch {epoch:02d} | \"\n        f\"Train Loss {train_loss:.4f} | \"\n        f\"Val Loss {val_loss:.4f} | \"\n        f\"Val Dice {val_dice:.4f}\"\n    )\n\n    if val_dice > best_dice:\n        best_dice = val_dice\n        torch.save(model.state_dict(), \"/kaggle/working/best_unetpp_effb4.pt\")\n        print(\">> Best model saved\")\n\nprint(\"\\n[STAGE 3 COMPLETE]\")\nprint(\"Best validation Dice:\", round(best_dice, 4))\nprint(\"Ready for STAGE 4 (Inference, Threshold Tuning & RLE Submission)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T00:41:07.103315Z","iopub.execute_input":"2026-02-05T00:41:07.104636Z","execution_failed":"2026-02-05T00:43:52.596Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device: cpu\nTotal training samples: 498\nTrain: 423 Valid: 75\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 74.4M/74.4M [00:00<00:00, 175MB/s]\nEpoch 1 [TRAIN]:   0%|          | 0/106 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\nEpoch 1 [TRAIN]:   6%|▌         | 6/106 [02:31<40:48, 24.49s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Optimization, Validation & Refinement","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — Optimization, Validation & Refinement\n# Purpose:\n# - Load best model\n# - Tune threshold on validation set\n# - Apply post-processing\n# - Select optimal configuration for inference\n# ============================================================\n\nimport numpy as np\nimport torch\nimport cv2\nfrom tqdm import tqdm\nimport pandas as pd\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nMODEL_PATH = \"/kaggle/working/best_unetpp_effb4.pt\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Threshold candidates (PB-oriented)\nTHRESHOLDS = [0.30, 0.35, 0.40, 0.45, 0.50]\n\n# Post-processing\nMIN_AREA = 300   # pixels; tune if needed\n\n# -----------------------------\n# LOAD MODEL\n# -----------------------------\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\nmodel.to(DEVICE)\n\nprint(\"[INFO] Best model loaded\")\n\n# -----------------------------\n# HELPER FUNCTIONS\n# -----------------------------\ndef dice_score(pred, target, eps=1e-7):\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum()\n    return (2 * intersection + eps) / (union + eps)\n\ndef remove_small_objects(mask, min_area):\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    clean = np.zeros_like(mask, dtype=np.uint8)\n    for i in range(1, num_labels):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            clean[labels == i] = 1\n    return clean\n\n# -----------------------------\n# THRESHOLD TUNING\n# -----------------------------\nresults = []\n\nwith torch.no_grad():\n    for thr in THRESHOLDS:\n        dices = []\n\n        for imgs, masks in tqdm(valid_loader, desc=f\"Tuning thr={thr}\"):\n            imgs = imgs.to(DEVICE)\n            masks = masks.to(DEVICE)\n\n            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n            gt = masks.cpu().numpy()\n\n            for i in range(probs.shape[0]):\n                pred = (probs[i, 0] > thr).astype(np.uint8)\n                pred = remove_small_objects(pred, MIN_AREA)\n\n                dice = dice_score(pred, gt[i, 0])\n                dices.append(dice)\n\n        mean_dice = float(np.mean(dices))\n        results.append({\"threshold\": thr, \"dice\": mean_dice})\n        print(f\"[RESULT] thr={thr:.2f} | Dice={mean_dice:.4f}\")\n\n# -----------------------------\n# SELECT BEST CONFIG\n# -----------------------------\ndf_thr = pd.DataFrame(results).sort_values(\"dice\", ascending=False)\nbest_thr = float(df_thr.iloc[0][\"threshold\"])\nbest_dice = float(df_thr.iloc[0][\"dice\"])\n\nprint(\"\\n[OPTIMAL CONFIG]\")\nprint(df_thr)\nprint(f\"\\nBest threshold : {best_thr}\")\nprint(f\"Best val Dice  : {best_dice:.4f}\")\n\n# -----------------------------\n# SAVE CONFIG FOR STAGE 5\n# -----------------------------\nOPT_CONFIG = {\n    \"threshold\": best_thr,\n    \"min_area\": MIN_AREA,\n}\n\nprint(\"\\n[STAGE 4 COMPLETE]\")\nprint(\"Threshold tuning finished\")\nprint(\"Post-processing calibrated\")\nprint(\"Ready for STAGE 5 (Inference & RLE Submission)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference, Encoding & Submission","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — Inference, RLE Encoding & Submission\n# Purpose:\n# - Run inference on test/images\n# - Apply threshold + post-processing\n# - Encode mask using RLE (column-wise)\n# - Generate submission.csv matching sample_submission\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport cv2\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# -----------------------------\n# CONFIG\n# -----------------------------\nDATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTEST_IMG_DIR = DATA_ROOT / \"test\" / \"images\"\nSAMPLE_SUB = Path(\"/kaggle/input/data-science-ara-7-0/sample_submission.csv\")\n\nMODEL_PATH = \"/kaggle/working/best_unetpp_effb4.pt\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# from STAGE 4\nBEST_THRESHOLD = OPT_CONFIG[\"threshold\"]\nMIN_AREA = OPT_CONFIG[\"min_area\"]\n\n# input size must match STAGE 2\nINPUT_SIZE = 512\n\n# -----------------------------\n# LOAD SAMPLE SUBMISSION\n# -----------------------------\ndf_sub = pd.read_csv(SAMPLE_SUB)\nid_col = df_sub.columns[0]   # ImageId\nrle_col = df_sub.columns[1]  # rle\n\nprint(\"[INFO] Sample submission loaded\")\nprint(df_sub.head())\n\n# -----------------------------\n# LOAD MODEL\n# -----------------------------\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\nmodel.to(DEVICE)\n\n# -----------------------------\n# RLE ENCODER (OFFICIAL)\n# -----------------------------\ndef encode_rle(mask: np.ndarray, pos_value: int = 255) -> str:\n    binary = (mask == pos_value).astype(np.uint8)\n    pixels = binary.T.flatten()  # column-wise\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[0::2]\n    return \" \".join(str(x) for x in runs)\n\n# -----------------------------\n# POST-PROCESSING\n# -----------------------------\ndef remove_small_objects(mask, min_area):\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    clean = np.zeros_like(mask, dtype=np.uint8)\n    for i in range(1, num_labels):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            clean[labels == i] = 1\n    return clean\n\n# -----------------------------\n# INFERENCE LOOP\n# -----------------------------\npred_rles = []\n\nwith torch.no_grad():\n    for img_name in tqdm(df_sub[id_col].values, desc=\"Inference\"):\n        img_path = TEST_IMG_DIR / img_name\n        assert img_path.exists(), f\"Missing test image {img_name}\"\n\n        # read image\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h0, w0 = img.shape[:2]\n\n        # resize (same as STAGE 2)\n        img_resized = cv2.resize(img, (INPUT_SIZE, INPUT_SIZE))\n        img_resized = img_resized.astype(\"float32\") / 255.0\n\n        # normalize ImageNet\n        img_resized[..., 0] = (img_resized[..., 0] - 0.485) / 0.229\n        img_resized[..., 1] = (img_resized[..., 1] - 0.456) / 0.224\n        img_resized[..., 2] = (img_resized[..., 2] - 0.406) / 0.225\n\n        # to tensor\n        img_tensor = torch.from_numpy(img_resized.transpose(2, 0, 1)).unsqueeze(0)\n        img_tensor = img_tensor.to(DEVICE)\n\n        # predict\n        prob = torch.sigmoid(model(img_tensor))[0, 0].cpu().numpy()\n\n        # threshold + post-process\n        pred = (prob > BEST_THRESHOLD).astype(np.uint8)\n        pred = remove_small_objects(pred, MIN_AREA)\n\n        # resize back to original size\n        pred = cv2.resize(pred, (w0, h0), interpolation=cv2.INTER_NEAREST)\n\n        # convert to {0,255}\n        pred_255 = pred * 255\n\n        # RLE\n        if pred_255.sum() == 0:\n            rle = \"\"\n        else:\n            rle = encode_rle(pred_255)\n\n        pred_rles.append(rle)\n\n# -----------------------------\n# BUILD SUBMISSION\n# -----------------------------\ndf_sub[rle_col] = pred_rles\n\nOUT_SUB = \"/kaggle/working/submission.csv\"\ndf_sub.to_csv(OUT_SUB, index=False)\n\nprint(\"\\n[STAGE 5 COMPLETE]\")\nprint(f\"Submission saved to: {OUT_SUB}\")\n\n# -----------------------------\n# FINAL QA\n# -----------------------------\nprint(\"\\n[QA CHECK]\")\nprint(\"Rows submission :\", len(df_sub))\nprint(\"Empty RLE count :\", (df_sub[rle_col] == \"\").sum())\nprint(\"Sample rows:\")\nprint(df_sub.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}