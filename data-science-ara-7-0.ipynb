{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":128328,"databundleVersionId":15445689,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":3935.89029,"end_time":"2026-02-05T21:13:18.335672","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-05T20:07:42.445382","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"120cd859542a4ad4914a275aa970c3ce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1556656a169c433faf0fcce78ce0c18c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"223f1b968d2e43c7a09bad50be0acdf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1fa80fce055417791cace1a6c61f72c","IPY_MODEL_27a16559e0864698a232fe1fe00e2df0","IPY_MODEL_2b6e0b5f64fd476bbe7660a79f157fba"],"layout":"IPY_MODEL_cbeae11723de45daa7bbb1da1bdc1dc2","tabbable":null,"tooltip":null}},"27a16559e0864698a232fe1fe00e2df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b39bdab99ac142e2be09b746d904869f","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1556656a169c433faf0fcce78ce0c18c","tabbable":null,"tooltip":null,"value":40}},"2b6e0b5f64fd476bbe7660a79f157fba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_629cbd2586334b23a726eed97daf3659","placeholder":"​","style":"IPY_MODEL_64e3be10918f48c1a6898ddb584bb72f","tabbable":null,"tooltip":null,"value":" 40/40 [04:58&lt;00:00,  7.45s/it]"}},"629cbd2586334b23a726eed97daf3659":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e3be10918f48c1a6898ddb584bb72f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"70cc7b9087a64bed8c59a7024c7dd314":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b39bdab99ac142e2be09b746d904869f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbeae11723de45daa7bbb1da1bdc1dc2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1fa80fce055417791cace1a6c61f72c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_120cd859542a4ad4914a275aa970c3ce","placeholder":"​","style":"IPY_MODEL_70cc7b9087a64bed8c59a7024c7dd314","tabbable":null,"tooltip":null,"value":"Best trial: 36. Best value: 0.75214: 100%"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Understanding & Preparation","metadata":{"papermill":{"duration":0.003014,"end_time":"2026-02-05T20:07:45.219228","exception":false,"start_time":"2026-02-05T20:07:45.216214","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Config & File Loading","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\nTRAIN_IMG_DIR = DATA_ROOT / \"train\" / \"images\"\nTRAIN_MASK_DIR = DATA_ROOT / \"train\" / \"mask\"\nTEST_IMG_DIR  = DATA_ROOT / \"test\" / \"images\"\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\"}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images = sorted([p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntrain_masks  = sorted([p for p in TRAIN_MASK_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\ntest_images  = sorted([p for p in TEST_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n\nprint(f\"[INFO] Train images : {len(train_images)}\")\nprint(f\"[INFO] Train masks  : {len(train_masks)}\")\nprint(f\"[INFO] Test images  : {len(test_images)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Pair Image Mask & Manifest","metadata":{}},{"cell_type":"code","source":"def extract_index(name: str):\n    m = re.search(r\"(\\d+)\", name)\n    return m.group(1) if m else None\n\n# Build mask index\nmask_index = {}\nfor m in train_masks:\n    idx = extract_index(m.stem)\n    if idx is not None:\n        mask_index[idx] = m\n\n# Pair image-mask\npairs = []\nfor img in train_images:\n    idx = extract_index(img.stem)\n    if idx in mask_index:\n        pairs.append({\n            \"image_path\": img,\n            \"mask_path\": mask_index[idx],\n            \"id\": idx\n        })\n\nassert len(pairs) > 0, \"No valid image-mask pairs found\"\nprint(f\"[INFO] Valid image-mask pairs: {len(pairs)}\")\n\n# Final manifest\ndf_manifest = pd.DataFrame({\n    \"image_path\": [str(p[\"image_path\"]) for p in pairs],\n    \"mask_path\":  [str(p[\"mask_path\"]) for p in pairs],\n    \"id\":         [p[\"id\"] for p in pairs],\n})\n\nprint(f\"[INFO] Final training samples: {len(df_manifest)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Morphology & Dice Riks Analysis","metadata":{}},{"cell_type":"code","source":"records = []\nall_component_areas = []\n\nfor p in tqdm(pairs, desc=\"Analyzing dataset\"):\n    mask = cv2.imread(str(p[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n    h, w = mask.shape\n    total_pixels = h * w\n\n    bin_mask = (mask == 255).astype(np.uint8)\n    pothole_pixels = bin_mask.sum()\n    area_ratio = pothole_pixels / total_pixels\n\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n        bin_mask, connectivity=8\n    )\n\n    component_areas = stats[1:, cv2.CC_STAT_AREA] if num_labels > 1 else []\n    if len(component_areas) > 0:\n        all_component_areas.extend(component_areas.tolist())\n\n    records.append({\n        \"image\": p[\"image_path\"].name,\n        \"height\": h,\n        \"width\": w,\n        \"has_pothole\": int(pothole_pixels > 0),\n        \"area_ratio\": area_ratio,\n        \"total_pothole_pixels\": pothole_pixels,\n        \"num_components\": len(component_areas),\n        \"max_component_ratio\": (\n            component_areas.max() / total_pixels if len(component_areas) > 0 else 0.0\n        ),\n        \"min_component_pixels\": (\n            component_areas.min() if len(component_areas) > 0 else 0\n        ),\n    })\n\ndf = pd.DataFrame(records)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Insight, Prior & Feasibility","metadata":{}},{"cell_type":"code","source":"print(\"\\n[INSIGHT] Pothole presence distribution:\")\nprint(df[\"has_pothole\"].value_counts())\n\nempty_ratio = (df[\"has_pothole\"] == 0).mean()\nprint(f\"\\n[INSIGHT] Empty-mask ratio: {empty_ratio:.2%}\")\n\nprint(\"\\n[INSIGHT] Pothole area ratio:\")\nprint(df[\"area_ratio\"].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n\nprint(\"\\n[INSIGHT] Number of components per image:\")\nprint(df[\"num_components\"].describe())\n\nprint(\"\\n[INSIGHT] Dominant component ratio:\")\nprint(df[\"max_component_ratio\"].describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Small-object analysis\ncomp_series = pd.Series(all_component_areas)\n\nprint(\"\\n[INSIGHT] Connected component area (pixels):\")\nprint(comp_series.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n\nmin_area_candidate = int(comp_series.quantile(0.10))\nprint(f\"\\n[RECOMMENDATION] Candidate MIN_AREA: ~{min_area_candidate} pixels\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dice feasibility\ntiny_image_ratio = (df[\"area_ratio\"] < 0.01).mean()\n\nprint(\"\\n[FEASIBILITY CHECK]\")\nprint(f\"Images with pothole <1% area: {tiny_image_ratio:.2%}\")\n\nif tiny_image_ratio > 0.6:\n    feasibility = \"HARD\"\nelif tiny_image_ratio > 0.4:\n    feasibility = \"MODERATE\"\nelse:\n    feasibility = \"FAVORABLE\"\n\nprint(f\"[FEASIBILITY STATUS] {feasibility}\")\n\nprint(\"\\n[THRESHOLD PRIOR]\")\nprint(\"→ Start sweep in range: 0.30 – 0.45\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing & Data Augmentation","metadata":{"papermill":{"duration":0.005254,"end_time":"2026-02-05T20:07:58.247873","exception":false,"start_time":"2026-02-05T20:07:58.242619","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Normalization & Base Config","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\nIMG_SIZE = 512\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Transform Factory","metadata":{}},{"cell_type":"code","source":"def build_base_transform():\n    return [\n        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_LINEAR),\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train Augmentation ","metadata":{}},{"cell_type":"code","source":"train_transform = A.Compose(\n    [\n        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_LINEAR),\n\n        # Geometry (Dice-safe)\n        A.HorizontalFlip(p=0.5),\n\n        A.Affine(\n            scale=(0.95, 1.07),\n            translate_percent=(0.0, 0.04),\n            rotate=(-3.0, 3.0),\n            shear=(-2.0, 2.0),\n            interpolation=cv2.INTER_LINEAR,\n            mode=cv2.BORDER_REFLECT_101,\n            p=0.45,\n        ),\n\n        # Photometric\n        A.RandomBrightnessContrast(\n            brightness_limit=0.20,\n            contrast_limit=0.20,\n            p=0.70,\n        ),\n\n        A.HueSaturationValue(\n            hue_shift_limit=6,\n            sat_shift_limit=12,\n            val_shift_limit=6,\n            p=0.35,\n        ),\n\n        # Shadow robustness\n        A.RandomShadow(\n            shadow_roi=(0, 0.5, 1, 1),\n            num_shadows_lower=1,\n            num_shadows_upper=2,\n            shadow_dimension=5,\n            p=0.25,\n        ),\n\n        # Mild noise / blur\n        A.OneOf(\n            [\n                A.GaussianBlur(blur_limit=3),\n                A.GaussNoise(var_limit=(4.0, 15.0)),\n            ],\n            p=0.18,\n        ),\n\n        # Normalize\n        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n        ToTensorV2(),\n    ],\n    additional_targets={\"mask\": \"mask\"},\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Valid & Test ","metadata":{}},{"cell_type":"code","source":"valid_transform = A.Compose(\n    build_base_transform(),\n    additional_targets={\"mask\": \"mask\"},\n)\n\ntest_transform = A.Compose(\n    build_base_transform()\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Construction & Training","metadata":{"papermill":{"duration":0.005001,"end_time":"2026-02-05T20:08:04.317894","exception":false,"start_time":"2026-02-05T20:08:04.312893","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch==0.3.3 timm","metadata":{"execution":{"iopub.status.busy":"2026-02-08T23:12:03.443653Z","iopub.execute_input":"2026-02-08T23:12:03.444419Z","iopub.status.idle":"2026-02-08T23:12:16.687781Z","shell.execute_reply.started":"2026-02-08T23:12:03.444389Z","shell.execute_reply":"2026-02-08T23:12:16.686544Z"},"papermill":{"duration":10.935761,"end_time":"2026-02-05T20:08:15.258582","exception":false,"start_time":"2026-02-05T20:08:04.322821","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Seed, Device, Split","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport segmentation_models_pytorch as smp\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\n\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# Use df_manifest from STAGE 1\ndf_train, df_val = train_test_split(\n    df_manifest,\n    test_size=0.15,\n    random_state=SEED,\n    shuffle=True\n)\n\nprint(\"Train:\", len(df_train), \"| Val:\", len(df_val))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dataset Class","metadata":{}},{"cell_type":"code","source":"class PotholeDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.df.loc[idx, \"image_path\"])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(self.df.loc[idx, \"mask_path\"], cv2.IMREAD_GRAYSCALE)\n        mask = (mask == 255).astype(\"float32\")\n\n        aug = self.transform(image=img, mask=mask)\n        return aug[\"image\"], aug[\"mask\"].unsqueeze(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loss, Metric, Model Factory","metadata":{}},{"cell_type":"code","source":"def dice_coef(prob, target, thr=0.40, eps=1e-7):\n    pred = (prob > thr).float()\n    inter = (pred * target).sum(dim=(2,3))\n    union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    return ((2 * inter + eps) / (union + eps)).mean()\n\ndice_loss = smp.losses.DiceLoss(mode=\"binary\", from_logits=True)\nfocal_loss = smp.losses.FocalLoss(mode=\"binary\", gamma=2.0)\n\ndef build_model(name):\n    if name == \"unetpp\":\n        return smp.UnetPlusPlus(\n            encoder_name=\"efficientnet-b4\",\n            encoder_weights=\"imagenet\",\n            in_channels=3,\n            classes=1,\n        )\n    elif name == \"deeplab\":\n        return smp.DeepLabV3Plus(\n            encoder_name=\"resnet101\",\n            encoder_weights=\"imagenet\",\n            in_channels=3,\n            classes=1,\n        )\n    else:\n        raise ValueError(\"Unknown model name\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train Loop","metadata":{}},{"cell_type":"code","source":"def train_one_model(name, max_epoch):\n\n    print(f\"\\n===== TRAINING {name.upper()} =====\")\n\n    model = build_model(name).to(device)\n\n    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=max_epoch)\n\n    train_loader = DataLoader(\n        PotholeDataset(df_train, train_transform),\n        batch_size=4,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        PotholeDataset(df_val, valid_transform),\n        batch_size=4,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    best_val = 0.0\n\n    for epoch in range(max_epoch):\n\n        # -------- TRAIN --------\n        model.train()\n        total_loss = 0.0\n\n        for imgs, masks in tqdm(train_loader, desc=f\"{name} | Epoch {epoch+1}\"):\n            imgs, masks = imgs.to(device), masks.to(device)\n\n            optimizer.zero_grad()\n\n            logits = model(imgs)\n            loss = dice_loss(logits, masks) + 0.6 * focal_loss(logits, masks)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        scheduler.step()\n        avg_loss = total_loss / len(train_loader)\n\n        # -------- VALID --------\n        model.eval()\n        dices = []\n\n        with torch.no_grad():\n            for imgs, masks in val_loader:\n                imgs, masks = imgs.to(device), masks.to(device)\n                prob = torch.sigmoid(model(imgs))\n                dices.append(dice_coef(prob, masks).item())\n\n        val_dice = float(np.mean(dices))\n\n        print(\n            f\"{name} | Epoch {epoch+1:02d} | \"\n            f\"TrainLoss {avg_loss:.4f} | ValDice {val_dice:.4f}\"\n        )\n\n        if val_dice > best_val:\n            best_val = val_dice\n            torch.save(model.state_dict(), f\"/kaggle/working/best_{name}.pt\")\n            print(f\">> Best {name} saved\")\n\n    print(f\"[DONE] {name} best Val Dice: {best_val:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run Training ","metadata":{}},{"cell_type":"code","source":"train_one_model(\"unetpp\", max_epoch=28)\ntrain_one_model(\"deeplab\", max_epoch=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimization, Validation & Refinement","metadata":{"papermill":{"duration":0.188839,"end_time":"2026-02-05T21:07:07.280193","exception":false,"start_time":"2026-02-05T21:07:07.091354","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Setup","metadata":{}},{"cell_type":"code","source":"import optuna\nimport numpy as np\nimport torch\nimport cv2\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\n# Use df_val from STAGE 3\nprint(\"[INFO] Validation samples:\", len(df_val))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Validation Loader ","metadata":{}},{"cell_type":"code","source":"val_loader = DataLoader(\n    PotholeDataset(df_val, valid_transform),\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load Models","metadata":{}},{"cell_type":"code","source":"unetpp = build_model(\"unetpp\").to(DEVICE)\ndeeplab = build_model(\"deeplab\").to(DEVICE)\n\nunetpp.load_state_dict(torch.load(\"/kaggle/working/best_unetpp.pt\", map_location=DEVICE))\ndeeplab.load_state_dict(torch.load(\"/kaggle/working/best_deeplab.pt\", map_location=DEVICE))\n\nunetpp.eval()\ndeeplab.eval()\n\nprint(\"Models loaded\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Metric & Postprocess","metadata":{}},{"cell_type":"code","source":"def dice_score(pred, target, eps=1e-7):\n    inter = (pred * target).sum()\n    union = pred.sum() + target.sum()\n    return (2 * inter + eps) / (union + eps)\n\ndef remove_small_objects(mask, min_area):\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask.astype(np.uint8), connectivity=8\n    )\n    clean = np.zeros_like(mask, dtype=np.uint8)\n    for i in range(1, num_labels):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            clean[labels == i] = 1\n    return clean","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Optuna Objective ","metadata":{}},{"cell_type":"code","source":"\n    w_u, w_d = w_u / s, w_d / s\n\n    threshold = trial.suggest_float(\"threshold\", 0.30, 0.45)\n    min_area  = trial.suggest_int(\"min_area\", 100, 400, step=20)\n\n    dices = []\n\n    with torch.no_grad():\n        for imgs, masks in val_loader:\n\n            imgs = imgs.to(DEVICE)\n            gt = masks.numpy()\n\n            pu = torch.sigmoid(unetpp(imgs)).cpu().numpy()\n            pd = torch.sigmoid(deeplab(imgs)).cpu().numpy()\n\n            prob = w_u * pu + w_d * pd\n\n            for i in range(prob.shape[0]):\n\n                pred = (prob[i, 0] > threshold).astype(np.uint8)\n                pred = remove_small_objects(pred, min_area)\n\n                dices.append(dice_score(pred, gt[i, 0]))\n\n    return float(np.mean(dices))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run Optuna ","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=40, show_progress_bar=True)\n\nbest = study.best_params\nbest_dice = study.best_value\n\n# normalize weights\nws = best[\"w_unetpp\"] + best[\"w_deeplab\"]\nbest[\"w_unetpp\"] /= ws\nbest[\"w_deeplab\"] /= ws\n\nOPT_CONFIG = {\n    \"weights\": {\n        \"unetpp\": best[\"w_unetpp\"],\n        \"deeplab\": best[\"w_deeplab\"],\n    },\n    \"threshold\": best[\"threshold\"],\n    \"min_area\": best[\"min_area\"],\n}\n\nprint(\"\\n[OPTUNA BEST CONFIG]\")\nfor k, v in OPT_CONFIG.items():\n    print(k, \":\", v)\n\nprint(f\"Validation Dice: {best_dice:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference, Encoding & Submission","metadata":{"papermill":{"duration":0.18696,"end_time":"2026-02-05T21:12:11.715168","exception":false,"start_time":"2026-02-05T21:12:11.528208","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Setup","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTEST_IMG_DIR = DATA_ROOT / \"test\" / \"images\"\nSAMPLE_SUB = Path(\"/kaggle/input/data-science-ara-7-0/sample_submission.csv\")\n\nW_U = OPT_CONFIG[\"weights\"][\"unetpp\"]\nW_D = OPT_CONFIG[\"weights\"][\"deeplab\"]\nTHRESHOLD = OPT_CONFIG[\"threshold\"]\nMIN_AREA = OPT_CONFIG[\"min_area\"]\n\nprint(\"Using optimized ensemble config\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load Models ","metadata":{}},{"cell_type":"code","source":"unetpp = build_model(\"unetpp\").to(DEVICE)\ndeeplab = build_model(\"deeplab\").to(DEVICE)\n\nunetpp.load_state_dict(torch.load(\"/kaggle/working/best_unetpp.pt\", map_location=DEVICE))\ndeeplab.load_state_dict(torch.load(\"/kaggle/working/best_deeplab.pt\", map_location=DEVICE))\n\nunetpp.eval()\ndeeplab.eval()\n\nprint(\"Models loaded\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test Transform","metadata":{}},{"cell_type":"code","source":"test_transform = A.Compose(\n    build_base_transform()\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"RLE","metadata":{}},{"cell_type":"code","source":"def encode_rle(mask: np.ndarray) -> str:\n    binary = (mask == 1).astype(np.uint8)\n    pixels = binary.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[0::2]\n    return \" \".join(str(x) for x in runs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final Ensemble INference","metadata":{}},{"cell_type":"code","source":"test_images = sorted(TEST_IMG_DIR.glob(\"*.jpg\"))\nrecords = []\n\nwith torch.no_grad():\n    for img_path in tqdm(test_images, desc=\"Final Inference\"):\n\n        img_name = img_path.name\n\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h0, w0 = img.shape[:2]\n\n        # apply same transform as training\n        aug = test_transform(image=img)\n        x = aug[\"image\"].unsqueeze(0).to(DEVICE)\n\n        x_flip = torch.flip(x, dims=[3])\n\n        # forward\n        p_u = torch.sigmoid(unetpp(x))\n        p_d = torch.sigmoid(deeplab(x))\n\n        p_u_f = torch.flip(torch.sigmoid(unetpp(x_flip)), dims=[3])\n        p_d_f = torch.flip(torch.sigmoid(deeplab(x_flip)), dims=[3])\n\n        p_u = (p_u + p_u_f) / 2.0\n        p_d = (p_d + p_d_f) / 2.0\n\n        prob = (W_U * p_u + W_D * p_d)[0, 0].cpu().numpy()\n\n        pred = (prob > THRESHOLD).astype(np.uint8)\n        pred = remove_small_objects(pred, MIN_AREA)\n\n        # resize back\n        pred = cv2.resize(pred, (w0, h0), interpolation=cv2.INTER_NEAREST)\n\n        rle = \"\" if pred.sum() == 0 else encode_rle(pred)\n\n        records.append({\n            \"ImageId\": img_name,\n            \"rle\": rle\n        })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Submission","metadata":{}},{"cell_type":"code","source":"df_sub = pd.DataFrame(records)\ndf_sample = pd.read_csv(SAMPLE_SUB)\n\ndf_sub = df_sub.set_index(\"ImageId\").loc[df_sample[\"ImageId\"]].reset_index()\n\nOUT_SUB = \"/kaggle/working/submission.csv\"\ndf_sub.to_csv(OUT_SUB, index=False)\n\nprint(\"Saved to:\", OUT_SUB)\nprint(\"Rows:\", len(df_sub))\nprint(\"Empty RLE:\", (df_sub[\"rle\"] == \"\").sum())\nprint(df_sub.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}