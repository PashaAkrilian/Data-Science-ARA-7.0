{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249e38c4",
   "metadata": {
    "papermill": {
     "duration": 0.004298,
     "end_time": "2026-02-09T12:12:10.642637",
     "exception": false,
     "start_time": "2026-02-09T12:12:10.638339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Understanding & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6f5720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T12:12:10.650612Z",
     "iopub.status.busy": "2026-02-09T12:12:10.650291Z",
     "iopub.status.idle": "2026-02-09T12:12:23.055448Z",
     "shell.execute_reply": "2026-02-09T12:12:23.054623Z"
    },
    "papermill": {
     "duration": 12.411459,
     "end_time": "2026-02-09T12:12:23.057240",
     "exception": false,
     "start_time": "2026-02-09T12:12:10.645781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train images : 498\n",
      "[INFO] Train masks  : 498\n",
      "[INFO] Test images  : 295\n",
      "[INFO] Valid image-mask pairs: 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing dataset: 100%|██████████| 498/498 [00:11<00:00, 44.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INSIGHT] Pothole presence:\n",
      "has_pothole\n",
      "1    498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INSIGHT] Area ratio (% image):\n",
      "count    498.000000\n",
      "mean       0.134860\n",
      "std        0.128772\n",
      "min        0.000235\n",
      "10%        0.007938\n",
      "25%        0.040943\n",
      "50%        0.091678\n",
      "75%        0.193834\n",
      "90%        0.329536\n",
      "max        0.674005\n",
      "Name: area_ratio, dtype: float64\n",
      "\n",
      "[INSIGHT] Bucket distribution:\n",
      "bucket\n",
      "large     0.691\n",
      "medium    0.191\n",
      "small     0.066\n",
      "tiny      0.052\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "[INSIGHT] Connected component area (px):\n",
      "count    2.122000e+03\n",
      "mean     5.588544e+04\n",
      "std      3.030841e+05\n",
      "min      1.000000e+00\n",
      "10%      1.301000e+02\n",
      "25%      3.930000e+02\n",
      "50%      1.913000e+03\n",
      "75%      1.203275e+04\n",
      "90%      5.370160e+04\n",
      "max      6.700584e+06\n",
      "dtype: float64\n",
      "\n",
      "[LOCKED PRIOR] MIN_AREA_PX ≈ 130\n",
      "\n",
      "[FEASIBILITY] <1% area images: 11.85%\n",
      "[FEASIBILITY STATUS] FAVORABLE\n",
      "\n",
      "[LOCKED THRESHOLD PRIOR]\n",
      "Use sweep range: 0.30 – 0.45\n",
      "\n",
      "[INFO] Final training samples: 498\n",
      "\n",
      "[STAGE 1 COMPLETE — LB-READY]\n",
      "✓ Dataset validated\n",
      "✓ Sampling buckets defined\n",
      "✓ Min-area & threshold locked\n",
      "✓ Manifest ready for STAGE 2/3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Data Understanding & Preparation (REVISED · LB-READY)\n",
    "# Purpose:\n",
    "# - Validate dataset integrity\n",
    "# - Quantify Dice risk factors (empty / tiny / fragmented)\n",
    "# - Derive data-driven priors for:\n",
    "#   • sampling strategy\n",
    "#   • min-area postprocess\n",
    "#   • threshold sweep\n",
    "# - Produce manifest for downstream stages\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\n",
    "TRAIN_IMG_DIR = DATA_ROOT / \"train/images\"\n",
    "TRAIN_MASK_DIR = DATA_ROOT / \"train/mask\"\n",
    "TEST_IMG_DIR  = DATA_ROOT / \"test/images\"\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "# -----------------------------\n",
    "# 1. LOAD FILES\n",
    "# -----------------------------\n",
    "train_images = sorted([p for p in TRAIN_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n",
    "train_masks  = sorted([p for p in TRAIN_MASK_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n",
    "test_images  = sorted([p for p in TEST_IMG_DIR.iterdir() if p.suffix.lower() in IMG_EXTS])\n",
    "\n",
    "print(f\"[INFO] Train images : {len(train_images)}\")\n",
    "print(f\"[INFO] Train masks  : {len(train_masks)}\")\n",
    "print(f\"[INFO] Test images  : {len(test_images)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. INDEX MASKS\n",
    "# -----------------------------\n",
    "def extract_index(name: str):\n",
    "    m = re.search(r\"(\\d+)\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "mask_index = {extract_index(m.stem): m for m in train_masks if extract_index(m.stem)}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. PAIR IMAGE–MASK\n",
    "# -----------------------------\n",
    "pairs = []\n",
    "for img in train_images:\n",
    "    idx = extract_index(img.stem)\n",
    "    if idx in mask_index:\n",
    "        pairs.append({\n",
    "            \"image_path\": img,\n",
    "            \"mask_path\": mask_index[idx],\n",
    "            \"id\": idx\n",
    "        })\n",
    "\n",
    "assert len(pairs) > 0\n",
    "print(f\"[INFO] Valid image-mask pairs: {len(pairs)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. MORPHOLOGY & DICE-RISK ANALYSIS\n",
    "# -----------------------------\n",
    "records = []\n",
    "all_component_areas = []\n",
    "\n",
    "for p in tqdm(pairs, desc=\"Analyzing dataset\"):\n",
    "    mask = cv2.imread(str(p[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n",
    "    h, w = mask.shape\n",
    "    total_pixels = h * w\n",
    "\n",
    "    bin_mask = (mask == 255).astype(np.uint8)\n",
    "    pothole_pixels = bin_mask.sum()\n",
    "    area_ratio = pothole_pixels / total_pixels\n",
    "\n",
    "    num_labels, _, stats, _ = cv2.connectedComponentsWithStats(\n",
    "        bin_mask, connectivity=8\n",
    "    )\n",
    "\n",
    "    component_areas = stats[1:, cv2.CC_STAT_AREA] if num_labels > 1 else []\n",
    "    if len(component_areas) > 0:\n",
    "        all_component_areas.extend(component_areas.tolist())\n",
    "\n",
    "    # bucket for stratified sampling\n",
    "    if pothole_pixels == 0:\n",
    "        bucket = \"empty\"\n",
    "    elif area_ratio < 0.002:\n",
    "        bucket = \"tiny\"\n",
    "    elif area_ratio < 0.01:\n",
    "        bucket = \"small\"\n",
    "    elif area_ratio < 0.05:\n",
    "        bucket = \"medium\"\n",
    "    else:\n",
    "        bucket = \"large\"\n",
    "\n",
    "    records.append({\n",
    "        \"image\": p[\"image_path\"].name,\n",
    "        \"image_path\": str(p[\"image_path\"]),\n",
    "        \"mask_path\": str(p[\"mask_path\"]),\n",
    "        \"height\": h,\n",
    "        \"width\": w,\n",
    "        \"has_pothole\": int(pothole_pixels > 0),\n",
    "        \"area_ratio\": area_ratio,\n",
    "        \"total_pothole_pixels\": pothole_pixels,\n",
    "        \"num_components\": len(component_areas),\n",
    "        \"max_component_pixels\": component_areas.max() if len(component_areas) > 0 else 0,\n",
    "        \"bucket\": bucket,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. CORE INSIGHTS\n",
    "# -----------------------------\n",
    "print(\"\\n[INSIGHT] Pothole presence:\")\n",
    "print(df[\"has_pothole\"].value_counts())\n",
    "\n",
    "print(\"\\n[INSIGHT] Area ratio (% image):\")\n",
    "print(df[\"area_ratio\"].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n",
    "\n",
    "print(\"\\n[INSIGHT] Bucket distribution:\")\n",
    "print(df[\"bucket\"].value_counts(normalize=True).round(3))\n",
    "\n",
    "# -----------------------------\n",
    "# 6. SMALL OBJECT ANALYSIS (POSTPROCESS PRIOR)\n",
    "# -----------------------------\n",
    "comp_series = pd.Series(all_component_areas)\n",
    "\n",
    "print(\"\\n[INSIGHT] Connected component area (px):\")\n",
    "print(comp_series.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n",
    "\n",
    "MIN_AREA_PX = int(comp_series.quantile(0.10))\n",
    "print(f\"\\n[LOCKED PRIOR] MIN_AREA_PX ≈ {MIN_AREA_PX}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. DICE FEASIBILITY SIGNAL\n",
    "# -----------------------------\n",
    "tiny_ratio = (df[\"area_ratio\"] < 0.01).mean()\n",
    "print(f\"\\n[FEASIBILITY] <1% area images: {tiny_ratio:.2%}\")\n",
    "\n",
    "if tiny_ratio > 0.6:\n",
    "    feasibility = \"HARD\"\n",
    "elif tiny_ratio > 0.4:\n",
    "    feasibility = \"MODERATE\"\n",
    "else:\n",
    "    feasibility = \"FAVORABLE\"\n",
    "\n",
    "print(f\"[FEASIBILITY STATUS] {feasibility}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. THRESHOLD PRIOR\n",
    "# -----------------------------\n",
    "THR_START, THR_END = 0.30, 0.45\n",
    "print(\"\\n[LOCKED THRESHOLD PRIOR]\")\n",
    "print(f\"Use sweep range: {THR_START:.2f} – {THR_END:.2f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. FINAL MANIFEST (DOWNSTREAM READY)\n",
    "# -----------------------------\n",
    "df_manifest = df[[\n",
    "    \"image_path\",\n",
    "    \"mask_path\",\n",
    "    \"has_pothole\",\n",
    "    \"area_ratio\",\n",
    "    \"bucket\"\n",
    "]].copy()\n",
    "\n",
    "print(f\"\\n[INFO] Final training samples: {len(df_manifest)}\")\n",
    "\n",
    "print(\"\\n[STAGE 1 COMPLETE — LB-READY]\")\n",
    "print(\"✓ Dataset validated\")\n",
    "print(\"✓ Sampling buckets defined\")\n",
    "print(\"✓ Min-area & threshold locked\")\n",
    "print(\"✓ Manifest ready for STAGE 2/3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc8180",
   "metadata": {
    "papermill": {
     "duration": 0.005905,
     "end_time": "2026-02-09T12:12:23.069626",
     "exception": false,
     "start_time": "2026-02-09T12:12:23.063721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing & Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba059ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T12:12:23.082490Z",
     "iopub.status.busy": "2026-02-09T12:12:23.082154Z",
     "iopub.status.idle": "2026-02-09T12:12:28.540643Z",
     "shell.execute_reply": "2026-02-09T12:12:28.539828Z"
    },
    "papermill": {
     "duration": 5.466911,
     "end_time": "2026-02-09T12:12:28.542250",
     "exception": false,
     "start_time": "2026-02-09T12:12:23.075339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE 2 FINAL — SHAPE-AWARE & 0.80-READY]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/653124267.py:31: UserWarning: Argument(s) 'mode' are not valid for transform Affine\n",
      "  A.Affine(\n",
      "/tmp/ipykernel_25/653124267.py:42: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
      "  A.ElasticTransform(\n",
      "/tmp/ipykernel_25/653124267.py:65: UserWarning: Argument(s) 'num_shadows_lower, num_shadows_upper' are not valid for transform RandomShadow\n",
      "  A.RandomShadow(\n",
      "/tmp/ipykernel_25/653124267.py:77: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(4.0, 12.0)),\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Preprocessing & Data Augmentation (FINAL · 0.80+)\n",
    "# ============================================================\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN AUGMENTATION — MASK-AWARE & SHAPE-ROBUST\n",
    "# ============================================================\n",
    "train_transform_512 = A.Compose(\n",
    "    [\n",
    "        # --- FIXED RESOLUTION ---\n",
    "        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n",
    "\n",
    "        # --- MASK-AWARE SPATIAL FOCUS (CRITICAL) ---\n",
    "        A.CropNonEmptyMaskIfExists(\n",
    "            height=448,\n",
    "            width=448,\n",
    "            p=0.40,\n",
    "        ),\n",
    "        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n",
    "\n",
    "        # --- SAFE GEOMETRY ---\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.Affine(\n",
    "            scale=(0.97, 1.05),\n",
    "            translate_percent=(0.0, 0.03),\n",
    "            rotate=(-2.5, 2.5),\n",
    "            shear=(-1.5, 1.5),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mode=cv2.BORDER_REFLECT_101,\n",
    "            p=0.30,\n",
    "        ),\n",
    "\n",
    "        # --- SHAPE DEFORMATION (KEY FOR 0.80) ---\n",
    "        A.ElasticTransform(\n",
    "            alpha=20,\n",
    "            sigma=6,\n",
    "            alpha_affine=4,\n",
    "            border_mode=cv2.BORDER_REFLECT_101,\n",
    "            p=0.25,\n",
    "        ),\n",
    "\n",
    "        # --- PHOTOMETRIC ---\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.18,\n",
    "            contrast_limit=0.18,\n",
    "            p=0.65,\n",
    "        ),\n",
    "\n",
    "        A.HueSaturationValue(\n",
    "            hue_shift_limit=5,\n",
    "            sat_shift_limit=10,\n",
    "            val_shift_limit=5,\n",
    "            p=0.30,\n",
    "        ),\n",
    "\n",
    "        # --- SHADOW (CONSERVATIVE) ---\n",
    "        A.RandomShadow(\n",
    "            shadow_roi=(0, 0.6, 1, 1),\n",
    "            num_shadows_lower=1,\n",
    "            num_shadows_upper=1,\n",
    "            shadow_dimension=4,\n",
    "            p=0.15,\n",
    "        ),\n",
    "\n",
    "        # --- VERY MILD NOISE ---\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.GaussianBlur(blur_limit=3),\n",
    "                A.GaussNoise(var_limit=(4.0, 12.0)),\n",
    "            ],\n",
    "            p=0.12,\n",
    "        ),\n",
    "\n",
    "        # --- NORMALIZE ---\n",
    "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    additional_targets={\"mask\": \"mask\"},\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# VALID / TEST (STRICT)\n",
    "# ============================================================\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n",
    "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    additional_targets={\"mask\": \"mask\"},\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n",
    "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"[STAGE 2 FINAL — SHAPE-AWARE & 0.80-READY]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e6fdd",
   "metadata": {
    "papermill": {
     "duration": 0.006423,
     "end_time": "2026-02-09T12:12:28.554972",
     "exception": false,
     "start_time": "2026-02-09T12:12:28.548549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Construction & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3f3ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T12:12:28.567926Z",
     "iopub.status.busy": "2026-02-09T12:12:28.567153Z",
     "iopub.status.idle": "2026-02-09T12:12:39.071879Z",
     "shell.execute_reply": "2026-02-09T12:12:39.071088Z"
    },
    "papermill": {
     "duration": 10.513038,
     "end_time": "2026-02-09T12:12:39.073662",
     "exception": false,
     "start_time": "2026-02-09T12:12:28.560624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q segmentation-models-pytorch==0.3.3 timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24166244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T12:12:39.088135Z",
     "iopub.status.busy": "2026-02-09T12:12:39.087862Z",
     "iopub.status.idle": "2026-02-09T14:42:49.258977Z",
     "shell.execute_reply": "2026-02-09T14:42:49.258026Z"
    },
    "papermill": {
     "duration": 9010.613633,
     "end_time": "2026-02-09T14:42:49.693748",
     "exception": false,
     "start_time": "2026-02-09T12:12:39.080115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_25/1040500572.py:57: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train: 423 | Val: 75\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b5-b6417697.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117M/117M [00:09<00:00, 13.0MB/s]\n",
      "Patch 1:   0%|          | 0/318 [00:00<?, ?it/s]/tmp/ipykernel_25/1040500572.py:210: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Patch 1: 100%|██████████| 318/318 [03:28<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 01 | Loss 162.1676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 2: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 02 | Loss 121.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 3: 100%|██████████| 318/318 [03:24<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 03 | Loss 112.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 4: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 04 | Loss 105.5874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 5: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 05 | Loss 89.7590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 6: 100%|██████████| 318/318 [03:24<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 06 | Loss 82.1440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 7: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 07 | Loss 81.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 8: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 08 | Loss 79.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 9: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 09 | Loss 75.4568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 10: 100%|██████████| 318/318 [03:24<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 10 | Loss 66.7382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 11: 100%|██████████| 318/318 [03:25<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 11 | Loss 64.2680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 12: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 12 | Loss 63.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 13: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 13 | Loss 52.3710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 14: 100%|██████████| 318/318 [03:25<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 14 | Loss 55.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 15: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 15 | Loss 56.4241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 16: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 16 | Loss 50.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 17: 100%|██████████| 318/318 [03:24<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 17 | Loss 48.6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 18: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 18 | Loss 45.7356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 19: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 19 | Loss 45.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 20: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 20 | Loss 44.2987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 21: 100%|██████████| 318/318 [03:24<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 21 | Loss 42.6991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 22: 100%|██████████| 318/318 [03:24<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 22 | Loss 42.4521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 23: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 23 | Loss 43.3448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 24: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 24 | Loss 38.7987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patch 25: 100%|██████████| 318/318 [03:24<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Epoch 25 | Loss 43.2907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 1:   0%|          | 0/212 [00:00<?, ?it/s]/tmp/ipykernel_25/1040500572.py:277: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Full 1: 100%|██████████| 212/212 [01:57<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 01 | ValDice 0.7124\n",
      ">> Best saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 2: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 02 | ValDice 0.7159\n",
      ">> Best saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 3: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 03 | ValDice 0.7167\n",
      ">> Best saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 4: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 04 | ValDice 0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 5: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 05 | ValDice 0.7175\n",
      ">> Best saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 6: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 06 | ValDice 0.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 7: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 07 | ValDice 0.7204\n",
      ">> Best saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 8: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 08 | ValDice 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 9: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 09 | ValDice 0.7145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 10: 100%|██████████| 212/212 [01:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 10 | ValDice 0.7205\n",
      ">> Best saved\n",
      "[INFO] Encoder unfrozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 11: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 11 | ValDice 0.7224\n",
      ">> Best saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 12: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 12 | ValDice 0.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 13: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 13 | ValDice 0.7211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 14: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 14 | ValDice 0.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 15: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 15 | ValDice 0.7120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 16: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 16 | ValDice 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 17: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 17 | ValDice 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full 18: 100%|██████████| 212/212 [04:14<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Epoch 18 | ValDice 0.7140\n",
      "\n",
      "[BEST VAL DICE] 0.7224\n",
      "[STAGE 3 DONE — 0.80 TRACK]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — PATCH + FULL IMAGE TRAINING (FINAL · 0.80 CORE)\n",
    "# UNet++ + EfficientNet-B5 | 768 RES | AMP SAFE | ONE CELL\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import random, re, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "IMG_SIZE = 768\n",
    "PATCH = 384\n",
    "\n",
    "PATCH_EPOCHS = 25\n",
    "FULL_EPOCHS  = 18\n",
    "\n",
    "PATCH_BATCH = 4\n",
    "FULL_BATCH  = 2\n",
    "ACCUM = 2\n",
    "\n",
    "LR_PATCH = 3e-4\n",
    "LR_FULL  = 1e-5\n",
    "\n",
    "FREEZE_EPOCHS = 10\n",
    "THR_RANGE = np.linspace(0.35, 0.50, 7)\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\n",
    "IMG_DIR = DATA_ROOT / \"train/images\"\n",
    "MSK_DIR = DATA_ROOT / \"train/mask\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD MANIFEST\n",
    "# ============================================================\n",
    "def extract_idx(name):\n",
    "    return re.search(r\"(\\d+)\", name).group(1)\n",
    "\n",
    "pairs = []\n",
    "for img in IMG_DIR.iterdir():\n",
    "    idx = extract_idx(img.name)\n",
    "    msk = MSK_DIR / f\"mask_{idx}.png\"\n",
    "    if msk.exists():\n",
    "        pairs.append((str(img), str(msk)))\n",
    "\n",
    "df = pd.DataFrame(pairs, columns=[\"image_path\", \"mask_path\"])\n",
    "df_train, df_val = train_test_split(\n",
    "    df, test_size=0.15, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(df_train), \"| Val:\", len(df_val))\n",
    "\n",
    "# ============================================================\n",
    "# DATASETS\n",
    "# ============================================================\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, df, tf):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tf = tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) * 3\n",
    "\n",
    "    def _safe_patch(self, img, mask, x1, y1):\n",
    "        h, w = img.shape[:2]\n",
    "        x1 = max(0, min(x1, w - PATCH))\n",
    "        y1 = max(0, min(y1, h - PATCH))\n",
    "        img_c = img[y1:y1+PATCH, x1:x1+PATCH]\n",
    "        mask_c = mask[y1:y1+PATCH, x1:x1+PATCH]\n",
    "        if img_c.shape[:2] != (PATCH, PATCH):\n",
    "            img_c = cv2.resize(img, (PATCH, PATCH))\n",
    "            mask_c = cv2.resize(mask, (PATCH, PATCH),\n",
    "                                interpolation=cv2.INTER_NEAREST)\n",
    "        return img_c, mask_c\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx % len(self.df)]\n",
    "        img = cv2.imread(row.image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(row.mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask == 255).astype(np.uint8)\n",
    "\n",
    "        if mask.sum() > 0 and random.random() < 0.7:\n",
    "            ys, xs = np.where(mask > 0)\n",
    "            i = random.randint(0, len(xs)-1)\n",
    "            img, mask = self._safe_patch(\n",
    "                img, mask,\n",
    "                xs[i] - PATCH//2,\n",
    "                ys[i] - PATCH//2\n",
    "            )\n",
    "        else:\n",
    "            h, w = img.shape[:2]\n",
    "            if h >= PATCH and w >= PATCH:\n",
    "                x1 = random.randint(0, w - PATCH)\n",
    "                y1 = random.randint(0, h - PATCH)\n",
    "                img, mask = self._safe_patch(img, mask, x1, y1)\n",
    "            else:\n",
    "                img = cv2.resize(img, (PATCH, PATCH))\n",
    "                mask = cv2.resize(mask, (PATCH, PATCH),\n",
    "                                  interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        aug = self.tf(image=img, mask=mask)\n",
    "        return aug[\"image\"], aug[\"mask\"].unsqueeze(0).float()\n",
    "\n",
    "\n",
    "class FullDataset(Dataset):\n",
    "    def __init__(self, df, tf):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tf = tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = cv2.imread(row.image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(row.mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask == 255).astype(np.float32)\n",
    "        aug = self.tf(image=img, mask=mask)\n",
    "        return aug[\"image\"], aug[\"mask\"].unsqueeze(0)\n",
    "\n",
    "# ============================================================\n",
    "# TRANSFORMS\n",
    "# ============================================================\n",
    "norm = dict(mean=(0.485,0.456,0.406),\n",
    "            std=(0.229,0.224,0.225))\n",
    "\n",
    "patch_tf = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(0.15, 0.15, p=0.5),\n",
    "    A.Normalize(**norm),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "full_train_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(**norm),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "full_val_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(**norm),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# MODEL & LOSS\n",
    "# ============================================================\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b5\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    ").to(DEVICE)\n",
    "\n",
    "dice  = smp.losses.DiceLoss(mode=\"binary\", from_logits=True)\n",
    "focal = smp.losses.FocalLoss(mode=\"binary\", gamma=2.0)\n",
    "\n",
    "# ============================================================\n",
    "# PHASE A — PATCH TRAINING\n",
    "# ============================================================\n",
    "patch_loader = DataLoader(\n",
    "    PatchDataset(df_train, patch_tf),\n",
    "    batch_size=PATCH_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=LR_PATCH, weight_decay=1e-4)\n",
    "sch = CosineAnnealingLR(opt, T_max=PATCH_EPOCHS)\n",
    "\n",
    "for e in range(1, PATCH_EPOCHS+1):\n",
    "    model.train()\n",
    "    tot = 0\n",
    "    for x,y in tqdm(patch_loader, desc=f\"Patch {e}\"):\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        with autocast():\n",
    "            loss = dice(model(x),y) + 0.5*focal(model(x),y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        tot += loss.item()\n",
    "    sch.step()\n",
    "    print(f\"Patch Epoch {e:02d} | Loss {tot:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# PHASE B — FULL IMAGE FINETUNE (768)\n",
    "# ============================================================\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "opt = AdamW(\n",
    "    [\n",
    "        {\"params\": model.encoder.parameters(), \"lr\": LR_FULL*0.1},\n",
    "        {\"params\": model.decoder.parameters(), \"lr\": LR_FULL},\n",
    "    ],\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "sch = CosineAnnealingLR(opt, T_max=FULL_EPOCHS)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    FullDataset(df_train, full_train_tf),\n",
    "    batch_size=FULL_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FullDataset(df_val, full_val_tf),\n",
    "    batch_size=FULL_BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    best = 0\n",
    "    for thr in THR_RANGE:\n",
    "        scores=[]\n",
    "        for x,y in val_loader:\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            p = (torch.sigmoid(model(x))>thr).float()\n",
    "            inter=(p*y).sum((2,3))\n",
    "            union=p.sum((2,3))+y.sum((2,3))\n",
    "            scores.append(((2*inter+1e-7)/(union+1e-7)).mean().item())\n",
    "        best=max(best,np.mean(scores))\n",
    "    return best\n",
    "\n",
    "best=0\n",
    "for e in range(1, FULL_EPOCHS+1):\n",
    "    model.train()\n",
    "    if e==FREEZE_EPOCHS+1:\n",
    "        for p in model.encoder.parameters():\n",
    "            p.requires_grad=True\n",
    "        print(\"[INFO] Encoder unfrozen\")\n",
    "\n",
    "    opt.zero_grad()\n",
    "    for i,(x,y) in enumerate(tqdm(train_loader, desc=f\"Full {e}\")):\n",
    "        x,y=x.to(DEVICE),y.to(DEVICE)\n",
    "        with autocast():\n",
    "            loss=(dice(model(x),y)+0.5*focal(model(x),y))/ACCUM\n",
    "        scaler.scale(loss).backward()\n",
    "        if (i+1)%ACCUM==0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad()\n",
    "\n",
    "    sch.step()\n",
    "    vd=validate()\n",
    "    print(f\"Full Epoch {e:02d} | ValDice {vd:.4f}\")\n",
    "    if vd>best:\n",
    "        best=vd\n",
    "        torch.save(model.state_dict(),\"/kaggle/working/unetpp_best.pt\")\n",
    "        print(\">> Best saved\")\n",
    "\n",
    "print(f\"\\n[BEST VAL DICE] {best:.4f}\")\n",
    "print(\"[STAGE 3 DONE — 0.80 TRACK]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa218f",
   "metadata": {
    "papermill": {
     "duration": 0.51422,
     "end_time": "2026-02-09T14:42:50.644556",
     "exception": false,
     "start_time": "2026-02-09T14:42:50.130336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Optimization, Validation & Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437ba99e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T14:42:51.506108Z",
     "iopub.status.busy": "2026-02-09T14:42:51.505403Z",
     "iopub.status.idle": "2026-02-09T14:45:47.426734Z",
     "shell.execute_reply": "2026-02-09T14:45:47.425973Z"
    },
    "papermill": {
     "duration": 176.350416,
     "end_time": "2026-02-09T14:45:47.429683",
     "exception": false,
     "start_time": "2026-02-09T14:42:51.079267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "[INFO] UNet++ B5 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-09 14:42:56,734] A new study created in memory with name: no-name-16deb1bc-12b5-41e9-81ac-886fa52fa094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] min_area range (768): 63 – 278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f7f15af44f434881ea053e73da3baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/optuna/distributions.py:684: UserWarning: The distribution is specified by [63, 278] and step=20, but the range is not divisible by `step`. It will be replaced with [63, 263].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-02-09 14:43:02,464] Trial 0 finished with value: 0.7181261204244572 and parameters: {'threshold': 0.38461919296619707, 'min_area': 163}. Best is trial 0 with value: 0.7181261204244572.\n",
      "[I 2026-02-09 14:43:08,153] Trial 1 finished with value: 0.7161471910682126 and parameters: {'threshold': 0.385768073452576, 'min_area': 243}. Best is trial 0 with value: 0.7181261204244572.\n",
      "[I 2026-02-09 14:43:13,823] Trial 2 finished with value: 0.7210021154742865 and parameters: {'threshold': 0.46305068596871246, 'min_area': 83}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:19,492] Trial 3 finished with value: 0.719751547902066 and parameters: {'threshold': 0.4564904856355993, 'min_area': 163}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:25,182] Trial 4 finished with value: 0.7189203091299673 and parameters: {'threshold': 0.3948085422506887, 'min_area': 123}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:30,920] Trial 5 finished with value: 0.7196029644283644 and parameters: {'threshold': 0.41539768751240547, 'min_area': 63}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:36,588] Trial 6 finished with value: 0.7197888999162134 and parameters: {'threshold': 0.4655075475057753, 'min_area': 163}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:42,264] Trial 7 finished with value: 0.7189087809536988 and parameters: {'threshold': 0.3672225229112599, 'min_area': 163}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:47,919] Trial 8 finished with value: 0.7179894889655144 and parameters: {'threshold': 0.5076927548647461, 'min_area': 223}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:53,589] Trial 9 finished with value: 0.7195686446268807 and parameters: {'threshold': 0.4434489735122549, 'min_area': 223}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:43:59,295] Trial 10 finished with value: 0.7199661610297468 and parameters: {'threshold': 0.4980502030147829, 'min_area': 63}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:44:05,010] Trial 11 finished with value: 0.7199735977381531 and parameters: {'threshold': 0.49905336699545944, 'min_area': 63}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:44:10,690] Trial 12 finished with value: 0.719814733241411 and parameters: {'threshold': 0.4837021041747526, 'min_area': 103}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:44:16,368] Trial 13 finished with value: 0.7194705090981924 and parameters: {'threshold': 0.5197089273142903, 'min_area': 103}. Best is trial 2 with value: 0.7210021154742865.\n",
      "[I 2026-02-09 14:44:22,049] Trial 14 finished with value: 0.7213517404462813 and parameters: {'threshold': 0.4838188975276636, 'min_area': 83}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:44:27,719] Trial 15 finished with value: 0.7198689304599475 and parameters: {'threshold': 0.47143646200768596, 'min_area': 123}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:44:33,399] Trial 16 finished with value: 0.7206076014340568 and parameters: {'threshold': 0.43029892295310873, 'min_area': 103}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:44:39,140] Trial 17 finished with value: 0.7198467329052295 and parameters: {'threshold': 0.4828892195568787, 'min_area': 123}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:44:44,842] Trial 18 finished with value: 0.7206360168462976 and parameters: {'threshold': 0.44187810446543285, 'min_area': 83}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:44:50,514] Trial 19 finished with value: 0.7191878604924673 and parameters: {'threshold': 0.41857330548189803, 'min_area': 203}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:44:56,202] Trial 20 finished with value: 0.7213459580040492 and parameters: {'threshold': 0.48190200374905173, 'min_area': 83}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:45:01,893] Trial 21 finished with value: 0.7210340714510962 and parameters: {'threshold': 0.47980351225614065, 'min_area': 83}. Best is trial 14 with value: 0.7213517404462813.\n",
      "[I 2026-02-09 14:45:07,580] Trial 22 finished with value: 0.7213567388652803 and parameters: {'threshold': 0.48344759791202807, 'min_area': 83}. Best is trial 22 with value: 0.7213567388652803.\n",
      "[I 2026-02-09 14:45:13,307] Trial 23 finished with value: 0.7198157766756254 and parameters: {'threshold': 0.4931049212810291, 'min_area': 143}. Best is trial 22 with value: 0.7213567388652803.\n",
      "[I 2026-02-09 14:45:19,000] Trial 24 finished with value: 0.7202612827557074 and parameters: {'threshold': 0.5188704274588148, 'min_area': 83}. Best is trial 22 with value: 0.7213567388652803.\n",
      "[I 2026-02-09 14:45:24,688] Trial 25 finished with value: 0.7215029539858879 and parameters: {'threshold': 0.4519374127045235, 'min_area': 103}. Best is trial 25 with value: 0.7215029539858879.\n",
      "[I 2026-02-09 14:45:30,355] Trial 26 finished with value: 0.7197398111288887 and parameters: {'threshold': 0.4529307179728123, 'min_area': 143}. Best is trial 25 with value: 0.7215029539858879.\n",
      "[I 2026-02-09 14:45:36,028] Trial 27 finished with value: 0.7215113117239667 and parameters: {'threshold': 0.4516840299767097, 'min_area': 103}. Best is trial 27 with value: 0.7215113117239667.\n",
      "[I 2026-02-09 14:45:41,691] Trial 28 finished with value: 0.7192200855857052 and parameters: {'threshold': 0.4245601107398428, 'min_area': 183}. Best is trial 27 with value: 0.7215113117239667.\n",
      "[I 2026-02-09 14:45:47,419] Trial 29 finished with value: 0.7197372838588254 and parameters: {'threshold': 0.4510274899276902, 'min_area': 143}. Best is trial 27 with value: 0.7215113117239667.\n",
      "\n",
      "[OPTUNA BEST CONFIG — FINAL]\n",
      "threshold: 0.4516840299767097\n",
      "min_area: 103\n",
      "Validation Dice: 0.7215\n",
      "\n",
      "[STAGE 4 COMPLETE — 0.80 TRACK]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Optimization & Refinement (FINAL · 0.80 SAFE)\n",
    "# UNet++ + EfficientNet-B5 | 768-SPACE | Dice-correct\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q optuna\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# -----------------------------\n",
    "# DEVICE\n",
    "# -----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ============================================================\n",
    "# VALIDATION SET (FROM STAGE 3)\n",
    "# ============================================================\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET\n",
    "# ============================================================\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, df, tf):\n",
    "        self.df = df\n",
    "        self.tf = tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.df.loc[idx, \"image_path\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.df.loc[idx, \"mask_path\"], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask == 255).astype(np.uint8)\n",
    "\n",
    "        aug = self.tf(image=img, mask=mask)\n",
    "        return aug[\"image\"], aug[\"mask\"]\n",
    "\n",
    "# ============================================================\n",
    "# TRANSFORM — STRICT (768)\n",
    "# ============================================================\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "VAL_SIZE = 768\n",
    "\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(VAL_SIZE, VAL_SIZE),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ValDataset(df_val, val_tf),\n",
    "    batch_size=2,          # VRAM-safe\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MODEL (MATCH STAGE 3)\n",
    "# ============================================================\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b5\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"/kaggle/working/unetpp_best.pt\", map_location=DEVICE)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"[INFO] UNet++ B5 loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# COMPUTE MIN_AREA RANGE (768-SPACE · PATCH-AWARE)\n",
    "# ============================================================\n",
    "areas = []\n",
    "\n",
    "for p in df_val[\"mask_path\"]:\n",
    "    m = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "    m = (m == 255).astype(np.uint8)\n",
    "    m = cv2.resize(m, (VAL_SIZE, VAL_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    n, _, stats, _ = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "    for i in range(1, n):\n",
    "        areas.append(stats[i, cv2.CC_STAT_AREA])\n",
    "\n",
    "areas = np.array(areas)\n",
    "\n",
    "MIN_AREA_LO = int(np.percentile(areas, 8))\n",
    "MIN_AREA_HI = int(np.percentile(areas, 30))\n",
    "\n",
    "print(f\"[INFO] min_area range (768): {MIN_AREA_LO} – {MIN_AREA_HI}\")\n",
    "\n",
    "# ============================================================\n",
    "# DICE — CORRECT (EMPTY-SAFE)\n",
    "# ============================================================\n",
    "def dice_correct(pred, gt, eps=1e-7):\n",
    "    if gt.sum() == 0 and pred.sum() == 0:\n",
    "        return 1.0\n",
    "    if gt.sum() == 0 and pred.sum() > 0:\n",
    "        return 0.0\n",
    "    inter = (pred * gt).sum()\n",
    "    union = pred.sum() + gt.sum()\n",
    "    return (2 * inter + eps) / (union + eps)\n",
    "\n",
    "def remove_small(mask, min_area):\n",
    "    n, labels, stats, _ = cv2.connectedComponentsWithStats(\n",
    "        mask.astype(np.uint8), connectivity=8\n",
    "    )\n",
    "    out = np.zeros_like(mask)\n",
    "    for i in range(1, n):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            out[labels == i] = 1\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# OPTUNA OBJECTIVE\n",
    "# ============================================================\n",
    "def objective(trial):\n",
    "    thr = trial.suggest_float(\"threshold\", 0.36, 0.52)\n",
    "    min_area = trial.suggest_int(\n",
    "        \"min_area\", MIN_AREA_LO, MIN_AREA_HI, step=20\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            probs = torch.sigmoid(model(imgs)).cpu().numpy()\n",
    "            masks = masks.numpy()\n",
    "\n",
    "            for i in range(len(probs)):\n",
    "                p = (probs[i, 0] > thr).astype(np.uint8)\n",
    "                p = remove_small(p, min_area)\n",
    "                scores.append(dice_correct(p, masks[i]))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "# ============================================================\n",
    "# RUN OPTUNA\n",
    "# ============================================================\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "best = study.best_params\n",
    "\n",
    "print(\"\\n[OPTUNA BEST CONFIG — FINAL]\")\n",
    "for k, v in best.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(f\"Validation Dice: {study.best_value:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT CONFIG\n",
    "# ============================================================\n",
    "OPT_CONFIG = {\n",
    "    \"weights\": {\"unetpp\": 1.0},\n",
    "    \"threshold\": best[\"threshold\"],\n",
    "    \"min_area\": best[\"min_area\"],\n",
    "    \"val_size\": VAL_SIZE,\n",
    "}\n",
    "\n",
    "print(\"\\n[STAGE 4 COMPLETE — 0.80 TRACK]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549656c1",
   "metadata": {
    "papermill": {
     "duration": 0.423696,
     "end_time": "2026-02-09T14:45:48.283881",
     "exception": false,
     "start_time": "2026-02-09T14:45:47.860185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference, Encoding & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5267af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T14:45:49.224496Z",
     "iopub.status.busy": "2026-02-09T14:45:49.223673Z",
     "iopub.status.idle": "2026-02-09T14:47:04.112050Z",
     "shell.execute_reply": "2026-02-09T14:47:04.110810Z"
    },
    "papermill": {
     "duration": 75.325582,
     "end_time": "2026-02-09T14:47:04.113819",
     "exception": false,
     "start_time": "2026-02-09T14:45:48.788237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] UNet++ B5 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multi-scale inference: 100%|██████████| 295/295 [01:14<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE 5 COMPLETE — MULTI-SCALE SUBMISSION READY]\n",
      "Saved to: /kaggle/working/submission.csv\n",
      "Total: 295\n",
      "Empty RLE: 2\n",
      "        ImageId                                                rle\n",
      "0  test_001.jpg  4343 2 4642 4 4941 6 5241 6 5540 7 5839 9 6139...\n",
      "1  test_002.jpg  69324 3 70039 2 70042 6 70758 10 71477 12 7219...\n",
      "2  test_003.jpg  576283 9 578579 9 580875 9 583171 9 585467 9 5...\n",
      "3  test_004.jpg                                                   \n",
      "4  test_005.jpg  48714 1 49013 3 49313 5 49612 12 49912 16 4993...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — FINAL MULTI-SCALE INFERENCE & SUBMISSION (0.80+)\n",
    "# UNet++ EffNet-B5 | 512 + 768 | LB-SAFE\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "DATA_ROOT = Path(\"/kaggle/input/data-science-ara-7-0/dataset/dataset\")\n",
    "TEST_IMG_DIR = DATA_ROOT / \"test/images\"\n",
    "SAMPLE_SUB = Path(\"/kaggle/input/data-science-ara-7-0/sample_submission.csv\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "THR = OPT_CONFIG[\"threshold\"]\n",
    "MIN_AREA_768 = OPT_CONFIG[\"min_area\"]\n",
    "\n",
    "SCALES = [512, 768]\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD MODEL (MATCH STAGE 3)\n",
    "# -----------------------------\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"efficientnet-b5\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"/kaggle/working/unetpp_best.pt\", map_location=DEVICE)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"[INFO] UNet++ B5 loaded\")\n",
    "\n",
    "# -----------------------------\n",
    "# TRANSFORM FACTORY\n",
    "# -----------------------------\n",
    "def build_tf(sz):\n",
    "    return A.Compose([\n",
    "        A.Resize(sz, sz),\n",
    "        A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# -----------------------------\n",
    "# RLE ENCODER\n",
    "# -----------------------------\n",
    "def encode_rle(mask):\n",
    "    pixels = mask.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[0::2]\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "# -----------------------------\n",
    "# POSTPROCESS\n",
    "# -----------------------------\n",
    "def remove_small(mask, min_area):\n",
    "    n, labels, stats, _ = cv2.connectedComponentsWithStats(\n",
    "        mask.astype(np.uint8), connectivity=8\n",
    "    )\n",
    "    out = np.zeros_like(mask)\n",
    "    for i in range(1, n):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            out[labels == i] = 1\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# INFERENCE\n",
    "# -----------------------------\n",
    "records = []\n",
    "test_images = sorted(TEST_IMG_DIR.glob(\"*.jpg\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_path in tqdm(test_images, desc=\"Multi-scale inference\"):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h0, w0 = img.shape[:2]\n",
    "\n",
    "        probs_all = []\n",
    "\n",
    "        for sz in SCALES:\n",
    "            tf = build_tf(sz)\n",
    "            x = tf(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "            x_f = torch.flip(x, dims=[3])\n",
    "\n",
    "            p = torch.sigmoid(model(x))\n",
    "            p_f = torch.flip(torch.sigmoid(model(x_f)), dims=[3])\n",
    "            prob = ((p + p_f) / 2.0)[0, 0].cpu().numpy()\n",
    "\n",
    "            prob = cv2.resize(prob, (768, 768))\n",
    "            probs_all.append(prob)\n",
    "\n",
    "        # ---- average multi-scale ----\n",
    "        prob_768 = np.mean(probs_all, axis=0)\n",
    "\n",
    "        # ---- global confidence guard ----\n",
    "        if prob_768.max() < THR * 0.85:\n",
    "            pred_768 = np.zeros((768, 768), dtype=np.uint8)\n",
    "        else:\n",
    "            pred_768 = (prob_768 > THR).astype(np.uint8)\n",
    "            pred_768 = remove_small(pred_768, MIN_AREA_768)\n",
    "\n",
    "        # ---- resize to original ----\n",
    "        pred = cv2.resize(\n",
    "            pred_768, (w0, h0), interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "\n",
    "        rle = \"\" if pred.sum() == 0 else encode_rle(pred)\n",
    "\n",
    "        records.append({\n",
    "            \"ImageId\": img_path.name,\n",
    "            \"rle\": rle\n",
    "        })\n",
    "\n",
    "# -----------------------------\n",
    "# SUBMISSION\n",
    "# -----------------------------\n",
    "df_sub = pd.DataFrame(records)\n",
    "df_sample = pd.read_csv(SAMPLE_SUB)\n",
    "df_sub = df_sub[df_sample.columns.tolist()]\n",
    "\n",
    "OUT_SUB = \"/kaggle/working/submission.csv\"\n",
    "df_sub.to_csv(OUT_SUB, index=False)\n",
    "\n",
    "print(\"\\n[STAGE 5 COMPLETE — MULTI-SCALE SUBMISSION READY]\")\n",
    "print(\"Saved to:\", OUT_SUB)\n",
    "print(\"Total:\", len(df_sub))\n",
    "print(\"Empty RLE:\", (df_sub['rle'] == '').sum())\n",
    "print(df_sub.head())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15445689,
     "sourceId": 128328,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9300.074251,
   "end_time": "2026-02-09T14:47:08.257665",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-09T12:12:08.183414",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "167bff8d87e348608fe085337d3b3cce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f9cd5e6b7d87499db213d50d88984a1c",
       "placeholder": "​",
       "style": "IPY_MODEL_821177cb0ff34402ad8de23ca73294aa",
       "tabbable": null,
       "tooltip": null,
       "value": " 30/30 [02:50&lt;00:00,  5.69s/it]"
      }
     },
     "32829e300d334e16ad9850d3ab8ac734": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "355652e8022f4eba91cdf733c7161563": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39ef19a8fb72484f9ad3c90d273d05ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f94c369eda8484f9cde7a5a8c2224b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ea707ac3b903480cbd2069b2a5f47eb7",
       "max": 30.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_32829e300d334e16ad9850d3ab8ac734",
       "tabbable": null,
       "tooltip": null,
       "value": 30.0
      }
     },
     "821177cb0ff34402ad8de23ca73294aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b82729346767400993e2e93ed230e029": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e5c1fd2d750d404c9eb1427968c9ac27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_39ef19a8fb72484f9ad3c90d273d05ee",
       "placeholder": "​",
       "style": "IPY_MODEL_b82729346767400993e2e93ed230e029",
       "tabbable": null,
       "tooltip": null,
       "value": "Best trial: 27. Best value: 0.721511: 100%"
      }
     },
     "ea707ac3b903480cbd2069b2a5f47eb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f7f7f15af44f434881ea053e73da3baa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e5c1fd2d750d404c9eb1427968c9ac27",
        "IPY_MODEL_5f94c369eda8484f9cde7a5a8c2224b7",
        "IPY_MODEL_167bff8d87e348608fe085337d3b3cce"
       ],
       "layout": "IPY_MODEL_355652e8022f4eba91cdf733c7161563",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f9cd5e6b7d87499db213d50d88984a1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
